# Workeråˆå§‹åŒ–å»¶è¿Ÿæ·±åº¦åˆ†æä¸Fargateè§£å†³æ–¹æ¡ˆ

**é—®é¢˜**: é¦–æ¬¡æŸ¥è¯¢è€—æ—¶~60ç§’ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†æ—¶é—´ç”¨äºåˆå§‹åŒ–Workers

**å½±å“**: Fargateé¢‘ç¹å¯åŠ¨/å…³é—­åœºæ™¯ä¸‹ï¼Œæ¯æ¬¡å†·å¯åŠ¨éƒ½ä¼šé¢ä¸´60ç§’å»¶è¿Ÿ

---

## ä¸€ã€é—®é¢˜åˆ†æ

### 1.1 å®é™…æµ‹è¯•æ•°æ®

ä»æµ‹è¯•æ—¥å¿—ä¸­è§‚å¯Ÿåˆ°çš„æ—¶é—´çº¿ï¼š

```
23:55:21 - æœåŠ¡å¯åŠ¨å®Œæˆ
23:55:55 - Embedding Workersåˆå§‹åŒ–ï¼ˆè€—æ—¶: ~30ç§’ï¼‰
23:56:25 - LLM Workersåˆå§‹åŒ–ï¼ˆè€—æ—¶: ~30ç§’ï¼‰
23:56:55 - æŸ¥è¯¢è¿”å›ç»“æœ

æ€»è€—æ—¶: ~60ç§’ï¼ˆé¦–æ¬¡æŸ¥è¯¢ï¼‰
åç»­æŸ¥è¯¢: 3-10ç§’ï¼ˆWorkerså·²é¢„çƒ­ï¼‰
```

### 1.2 æ—¥å¿—è¯æ®

```log
INFO: Embedding func: 8 new workers initialized (Timeouts: Func: 30s, Worker: 60s, Health Check: 75s)
INFO: Naive query: 5 chunks (chunk_top_k:10 cosine:0.2)
INFO: Successfully reranked: 5 chunks from 5 original chunks
INFO: Final context: 5 chunks
INFO: LLM func: 8 new workers initialized (Timeouts: Func: 180s, Worker: 360s, Health Check: 375s)
```

### 1.3 æ ¹æœ¬åŸå› 

#### Workeræ± è®¾è®¡ï¼ˆLightRAGæºç ï¼‰

æŸ¥çœ‹LightRAGçš„workerå®ç°ï¼ˆæ¨æµ‹åŸºäºæ—¥å¿—ï¼‰ï¼š

1. **æƒ°æ€§åˆå§‹åŒ–ï¼ˆLazy Initializationï¼‰**
   - Workersä¸åœ¨æœåŠ¡å¯åŠ¨æ—¶é¢„çƒ­
   - é¦–æ¬¡æŸ¥è¯¢è§¦å‘æ—¶æ‰åˆ›å»ºWorkeræ± 
   - ç›®çš„ï¼šèŠ‚çœå†…å­˜å’Œå¯åŠ¨æ—¶é—´

2. **Workeræ•°é‡**
   - Embedding Workers: 8ä¸ª
   - LLM Workers: 8ä¸ª
   - æ¯ä¸ªWorkeréƒ½éœ€è¦ç‹¬ç«‹åˆå§‹åŒ–

3. **åˆå§‹åŒ–æµç¨‹**
   ```
   é¦–æ¬¡æŸ¥è¯¢ â†’ æ£€æµ‹æ— Workers â†’ åˆ›å»ºWorkeræ±  â†’
   åˆå§‹åŒ–è¿æ¥ â†’ Health Check â†’ å¼€å§‹å¤„ç†
   ```

4. **ä¸ºä»€ä¹ˆéœ€è¦30ç§’ï¼Ÿ**

   **Embedding Workers (30ç§’)**:
   ```python
   - åˆ›å»º8ä¸ªå¼‚æ­¥Workerè¿›ç¨‹
   - æ¯ä¸ªWorkerè¿æ¥åˆ°SF Embedding API
   - å»ºç«‹HTTPè¿æ¥æ± 
   - æ‰§è¡Œé¦–æ¬¡è°ƒç”¨éªŒè¯
   - Health Checkè¶…æ—¶: 75ç§’
   ```

   **LLM Workers (30ç§’)**:
   ```python
   - åˆ›å»º8ä¸ªå¼‚æ­¥Workerè¿›ç¨‹
   - æ¯ä¸ªWorkerè¿æ¥åˆ°ARK LLM API
   - å»ºç«‹HTTPè¿æ¥æ± ï¼ˆæ”¯æŒstreamingï¼‰
   - æ‰§è¡Œé¦–æ¬¡è°ƒç”¨éªŒè¯
   - Health Checkè¶…æ—¶: 375ç§’
   ```

---

## äºŒã€Fargateåœºæ™¯ä¸‹çš„æŒ‘æˆ˜

### 2.1 Fargateå®¹å™¨ç”Ÿå‘½å‘¨æœŸ

```
è¯·æ±‚åˆ°è¾¾ â†’ å†·å¯åŠ¨å®¹å™¨ (5-15ç§’) â†’
åº”ç”¨å¯åŠ¨ (5-10ç§’) â†’ é¦–æ¬¡è¯·æ±‚ (60ç§’å»¶è¿Ÿ) â†’
æ— è¯·æ±‚è¶…æ—¶ (15åˆ†é’Ÿ) â†’ å®¹å™¨å…³é—­ â†’ é‡å¤å¾ªç¯
```

### 2.2 å®é™…å½±å“

| åœºæ™¯ | å†·å¯åŠ¨é¢‘ç‡ | ç”¨æˆ·ä½“éªŒ | æˆæœ¬å½±å“ |
|------|-----------|---------|---------|
| **ä½æµé‡** | é¢‘ç¹ | æ¯æ¬¡ç­‰å¾…75-85ç§’ | âŒ å·® |
| **ä¸­æµé‡** | å¶å°” | å¶å°”å»¶è¿Ÿ | âš ï¸  ä¸€èˆ¬ |
| **é«˜æµé‡** | å¾ˆå°‘ | å»¶è¿Ÿç¨³å®š | âœ… è‰¯å¥½ |

### 2.3 æˆæœ¬åˆ†æ

å‡è®¾ï¼š
- Fargateå®¹å™¨: 0.5 vCPU, 1GB RAM
- ä»·æ ¼: ~$0.01 / vCPU-hour + $0.001 / GB-hour
- æ¯å°æ—¶è¯·æ±‚: 10æ¬¡ï¼ˆä½æµé‡ï¼‰

**å½“å‰æ¶æ„**:
- æ¯æ¬¡å†·å¯åŠ¨: 75-85ç§’
- å®¹å™¨é—²ç½®æ—¶é—´: 80% (15åˆ†é’Ÿè¶…æ—¶é¢‘ç¹è§¦å‘)
- **æœ‰æ•ˆåˆ©ç”¨ç‡**: ~20%
- **æœˆæˆæœ¬**: ~$10-15ï¼ˆå¤§éƒ¨åˆ†æµªè´¹åœ¨ç­‰å¾…ï¼‰

**ä¼˜åŒ–å**:
- å†·å¯åŠ¨: 10-15ç§’
- å®¹å™¨é—²ç½®æ—¶é—´: 50%
- **æœ‰æ•ˆåˆ©ç”¨ç‡**: ~50%
- **æœˆæˆæœ¬**: ~$8-10ï¼ˆæ›´é«˜ROIï¼‰

---

## ä¸‰ã€è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: é¢„çƒ­Workersï¼ˆæ¨èï¼‰â­â­â­â­â­

**åŸç†**: åœ¨æœåŠ¡å¯åŠ¨æ—¶ç«‹å³åˆå§‹åŒ–Workers

**å®æ–½æ–¹æ³•**:

#### ä¿®æ”¹ LightRAG åˆå§‹åŒ–æµç¨‹

```python
# src/rag.py:lifespan() æ·»åŠ é¢„çƒ­é€»è¾‘

async def lifespan(app: FastAPI):
    # ç°æœ‰åˆå§‹åŒ–ä»£ç ...

    # ğŸ”¥ æ–°å¢: Workersé¢„çƒ­
    logger.info("Warming up Workers...")

    # 1. é¢„çƒ­Embedding Workers
    try:
        test_embedding = await global_lightrag_instance.embedding_func(
            ["warmup test"]
        )
        logger.info(f"âœ“ Embedding Workers warmed up ({len(test_embedding)} dim)")
    except Exception as e:
        logger.warning(f"Embedding warmup failed: {e}")

    # 2. é¢„çƒ­LLM Workers
    try:
        test_response = await global_lightrag_instance.llm_model_func(
            "Hello"
        )
        logger.info(f"âœ“ LLM Workers warmed up ({len(test_response)} chars)")
    except Exception as e:
        logger.warning(f"LLM warmup failed: {e}")

    logger.info("âœ… All Workers ready for requests")

    yield  # åº”ç”¨è¿è¡Œ
```

**é¢„æœŸæ•ˆæœ**:
- âœ… æœåŠ¡å¯åŠ¨æ—¶é—´: 5-10ç§’ â†’ 35-45ç§’
- âœ… é¦–æ¬¡æŸ¥è¯¢å»¶è¿Ÿ: 60ç§’ â†’ 3-10ç§’
- âœ… Fargateå®¹å™¨å¯åŠ¨æ€»æ—¶é—´: 85ç§’ â†’ 55ç§’ï¼ˆ**èŠ‚çœ35%**ï¼‰

**ä¼˜ç‚¹**:
- ğŸ¯ ç›´æ¥è§£å†³é—®é¢˜
- ğŸ“Š ç”¨æˆ·ä½“éªŒå¤§å¹…æå‡
- ğŸ’° Fargateåˆ©ç”¨ç‡æé«˜
- ğŸ”§ å®æ–½ç®€å•ï¼ˆ~10è¡Œä»£ç ï¼‰

**ç¼ºç‚¹**:
- â³ æœåŠ¡å¯åŠ¨æ…¢30-40ç§’
- ğŸ’¸ å³ä½¿æ— è¯·æ±‚ä¹Ÿæ¶ˆè€—èµ„æº

---

### æ–¹æ¡ˆ2: å‡å°‘Workeræ•°é‡

**åŸç†**: é™ä½Workeræ•°é‡ï¼Œå‡å°‘åˆå§‹åŒ–å¼€é”€

**å®æ–½æ–¹æ³•**:

```python
# .env é…ç½®
MAX_ASYNC=4  # ä»8é™åˆ°4

# æˆ–è€…ä½¿ç”¨ç¯å¢ƒå˜é‡åŠ¨æ€è°ƒæ•´
MAX_ASYNC=${WORKER_COUNT:-4}
```

**é¢„æœŸæ•ˆæœ**:
- Workeråˆå§‹åŒ–æ—¶é—´: 30ç§’ â†’ 15-20ç§’
- é¦–æ¬¡æŸ¥è¯¢å»¶è¿Ÿ: 60ç§’ â†’ 30-40ç§’ï¼ˆ**èŠ‚çœ33-50%**ï¼‰

**trade-offs**:
- âš ï¸ å¹¶å‘æŸ¥è¯¢æ€§èƒ½ä¸‹é™
- âš ï¸ é«˜æµé‡æ—¶å¯èƒ½ç“¶é¢ˆ
- âœ… ä½æµé‡åœºæ™¯æ›´ç»æµ

---

### æ–¹æ¡ˆ3: æŒ‰éœ€åˆå§‹åŒ– + å¿«é€Ÿå¤±è´¥

**åŸç†**: å¼‚æ­¥åˆå§‹åŒ–ï¼Œä¸é˜»å¡é¦–æ¬¡è¯·æ±‚

**å®æ–½æ–¹æ³•**:

```python
# åˆ›å»ºå…¨å±€Workeræ± ç®¡ç†å™¨
class WorkerPoolManager:
    def __init__(self):
        self.embedding_ready = False
        self.llm_ready = False
        self.init_task = None

    async def initialize_async(self):
        """åå°å¼‚æ­¥åˆå§‹åŒ–"""
        logger.info("Starting async worker initialization...")

        # å¹¶è¡Œåˆå§‹åŒ–
        await asyncio.gather(
            self._init_embedding_workers(),
            self._init_llm_workers()
        )

        self.embedding_ready = True
        self.llm_ready = True
        logger.info("Workers ready!")

    async def _init_embedding_workers(self):
        # è§¦å‘ä¸€æ¬¡embeddingè°ƒç”¨
        await global_lightrag_instance.embedding_func(["warmup"])

    async def _init_llm_workers(self):
        # è§¦å‘ä¸€æ¬¡LLMè°ƒç”¨
        await global_lightrag_instance.llm_model_func("Hello")

# åœ¨lifespanä¸­å¯åŠ¨
worker_manager = WorkerPoolManager()

async def lifespan(app: FastAPI):
    # ç°æœ‰åˆå§‹åŒ–...

    # å¯åŠ¨åå°åˆå§‹åŒ–ä»»åŠ¡ï¼ˆä¸ç­‰å¾…ï¼‰
    asyncio.create_task(worker_manager.initialize_async())

    yield

# åœ¨æŸ¥è¯¢APIä¸­æ£€æŸ¥çŠ¶æ€
@router.post("/query")
async def query_rag(request: QueryRequest):
    if not worker_manager.embedding_ready or not worker_manager.llm_ready:
        # è¿”å›202 Accepted + è¿›åº¦ä¿¡æ¯
        return JSONResponse(
            status_code=202,
            content={
                "status": "initializing",
                "message": "Workers are warming up, please retry in 10-20s",
                "embedding_ready": worker_manager.embedding_ready,
                "llm_ready": worker_manager.llm_ready
            }
        )

    # æ­£å¸¸å¤„ç†æŸ¥è¯¢...
```

**é¢„æœŸæ•ˆæœ**:
- æœåŠ¡å¯åŠ¨æ—¶é—´: ä¸å˜ï¼ˆ5-10ç§’ï¼‰
- é¦–æ¬¡æŸ¥è¯¢: è¿”å›202æç¤ºæ­£åœ¨åˆå§‹åŒ–
- åç»­æŸ¥è¯¢ï¼ˆ10-20ç§’åï¼‰: æ­£å¸¸å“åº”

**ä¼˜ç‚¹**:
- âœ… æœåŠ¡å¯åŠ¨å¿«
- âœ… ç”¨æˆ·å¾—åˆ°æ˜ç¡®åé¦ˆ
- âœ… è‡ªåŠ¨é‡è¯•å‹å¥½

**ç¼ºç‚¹**:
- âš ï¸ éœ€è¦å®¢æˆ·ç«¯æ”¯æŒ202çŠ¶æ€ç 
- âš ï¸ ç”¨æˆ·ä½“éªŒç¨å·®ï¼ˆéœ€è¦é‡è¯•ï¼‰

---

### æ–¹æ¡ˆ4: Keep-Aliveç­–ç•¥

**åŸç†**: å»¶é•¿Fargateå®¹å™¨ç”Ÿå‘½å‘¨æœŸï¼Œå‡å°‘å†·å¯åŠ¨

**å®æ–½æ–¹æ³•**:

```python
# 1. æ·»åŠ å¿ƒè·³ç«¯ç‚¹
@app.get("/health")
async def health_check():
    return {"status": "healthy", "workers_ready": True}

# 2. é…ç½®å¤–éƒ¨å®šæ—¶å™¨ï¼ˆAWS CloudWatch Eventsï¼‰
# æ¯10åˆ†é’Ÿè°ƒç”¨ä¸€æ¬¡ /healthï¼Œä¿æŒå®¹å™¨æ´»è·ƒ
```

```yaml
# CloudWatch Eventsè§„åˆ™
ScheduleExpression: rate(10 minutes)
Target: http://45.78.223.205:8000/health
```

**é¢„æœŸæ•ˆæœ**:
- å®¹å™¨é—²ç½®è¶…æ—¶: 15åˆ†é’Ÿ â†’ æ— é™ï¼ˆæœ‰å¿ƒè·³ï¼‰
- å†·å¯åŠ¨é¢‘ç‡: é¢‘ç¹ â†’ å¾ˆå°‘

**æˆæœ¬å½±å“**:
- ğŸ’¸ å®¹å™¨æŒç»­è¿è¡Œï¼ˆå³ä½¿æ— çœŸå®è¯·æ±‚ï¼‰
- ğŸ’° æœˆæˆæœ¬å¢åŠ : +50-100%
- âš–ï¸  ä»…é€‚åˆä¸­é«˜æµé‡åœºæ™¯

---

### æ–¹æ¡ˆ5: Lambda@Edge ç¼“å­˜é¢„çƒ­

**åŸç†**: ä½¿ç”¨è¾¹ç¼˜å‡½æ•°åœ¨å®¹å™¨å†·å¯åŠ¨åç«‹å³é¢„çƒ­

**å®æ–½æ–¹æ³•**:

```python
# Lambda@Edgeå‡½æ•°
import json
import urllib3

def lambda_handler(event, context):
    # æ£€æµ‹å®¹å™¨å†·å¯åŠ¨
    if is_cold_start():
        # å‘é€é¢„çƒ­è¯·æ±‚
        http = urllib3.PoolManager()
        http.request('POST', 'http://backend/warmup', timeout=1.0)

        # è¿”å›ç­‰å¾…æç¤º
        return {
            'statusCode': 503,
            'body': json.dumps({
                'message': 'Service warming up, retry in 30s'
            })
        }

    # è½¬å‘æ­£å¸¸è¯·æ±‚
    return proxy_to_backend(event)
```

**ä¼˜ç‚¹**:
- ğŸŒ è¾¹ç¼˜é¢„çƒ­ï¼Œå‡å°‘å»¶è¿Ÿ
- ğŸ¯ å¯¹ç”¨æˆ·é€æ˜

**ç¼ºç‚¹**:
- ğŸ”§ å®æ–½å¤æ‚åº¦é«˜
- ğŸ’¸ Lambdaæˆæœ¬é¢å¤–å¼€é”€

---

## å››ã€æ¨èæ–¹æ¡ˆ

### ç«‹å³å®æ–½ï¼ˆPhase 1ï¼‰: æ–¹æ¡ˆ1 + æ–¹æ¡ˆ2

**ç†ç”±**:
- æœ€å¤§åŒ–é™ä½å†·å¯åŠ¨å»¶è¿Ÿ
- å®æ–½ç®€å•ï¼Œé£é™©ä½
- æˆæœ¬å¯æ§

**å®æ–½ä»£ç **:

```python
# src/rag.py ä¿®æ”¹

# 1. å‡å°‘Workeræ•°é‡
MAX_ASYNC = int(os.getenv("MAX_ASYNC", "4"))  # ä»8é™åˆ°4

# 2. æ·»åŠ é¢„çƒ­é€»è¾‘
async def lifespan(app: FastAPI):
    # ... ç°æœ‰åˆå§‹åŒ–ä»£ç  ...

    logger.info("ğŸ”¥ Warming up Workers...")
    start_time = time.time()

    try:
        # å¹¶è¡Œé¢„çƒ­
        await asyncio.gather(
            global_lightrag_instance.embedding_func(["warmup test"]),
            global_lightrag_instance.llm_model_func("Hello"),
            return_exceptions=True
        )

        elapsed = time.time() - start_time
        logger.info(f"âœ… Workers warmed up in {elapsed:.2f}s")
    except Exception as e:
        logger.warning(f"âš ï¸  Warmup failed (will retry on first request): {e}")

    yield
```

```.env
# ç¯å¢ƒå˜é‡é…ç½®
MAX_ASYNC=4  # é™ä½Workeræ•°é‡
```

**é¢„æœŸæ•ˆæœ**:

| æŒ‡æ ‡ | ä¿®æ”¹å‰ | ä¿®æ”¹å | æ”¹å–„ |
|------|--------|--------|------|
| æœåŠ¡å¯åŠ¨æ—¶é—´ | 10ç§’ | 25ç§’ | -150% |
| é¦–æ¬¡æŸ¥è¯¢å»¶è¿Ÿ | 60ç§’ | 5-10ç§’ | **-83%** |
| Fargateå†·å¯åŠ¨æ€»æ—¶é—´ | 85ç§’ | 45ç§’ | **-47%** |
| å¹¶å‘æŸ¥è¯¢æ€§èƒ½ | 8å¹¶å‘ | 4å¹¶å‘ | -50% |
| æœˆæˆæœ¬ï¼ˆä½æµé‡ï¼‰ | $12 | $8 | **-33%** |

---

### åç»­ä¼˜åŒ–ï¼ˆPhase 2ï¼‰: æ–¹æ¡ˆ4

**æ¡ä»¶**: æµé‡å¢é•¿åˆ°10+ req/hour

**å®æ–½**: æ·»åŠ CloudWatchå¿ƒè·³ï¼Œä¿æŒå®¹å™¨æ´»è·ƒ

---

## äº”ã€Fargateç‰¹å®šé…ç½®å»ºè®®

### 5.1 Fargate Task Definition

```json
{
  "containerDefinitions": [{
    "name": "rag-api",
    "cpu": 1024,        // 1 vCPU
    "memory": 2048,     // 2 GB (Workeråˆå§‹åŒ–éœ€è¦æ›´å¤šå†…å­˜)
    "environment": [
      {"name": "MAX_ASYNC", "value": "4"},
      {"name": "WARMUP_ON_STARTUP", "value": "true"}
    ],
    "healthCheck": {
      "command": ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"],
      "interval": 30,
      "timeout": 5,
      "retries": 3,
      "startPeriod": 60  // ç»™äºˆ60ç§’å¯åŠ¨+é¢„çƒ­æ—¶é—´
    }
  }]
}
```

### 5.2 Auto Scalingé…ç½®

```json
{
  "targetTrackingScaling": {
    "targetValue": 70.0,  // CPUåˆ©ç”¨ç‡ç›®æ ‡
    "scaleOutCooldown": 30,  // æ‰©å®¹å†·å´æ—¶é—´ï¼ˆç§’ï¼‰
    "scaleInCooldown": 300   // ç¼©å®¹å†·å´æ—¶é—´ï¼ˆ5åˆ†é’Ÿï¼‰
  },
  "minCapacity": 1,  // æœ€å°å®¹å™¨æ•°ï¼ˆä¿æŒ1ä¸ªé¢„çƒ­ï¼‰
  "maxCapacity": 10
}
```

### 5.3 ALB Target Groupé…ç½®

```json
{
  "healthCheckPath": "/health",
  "healthCheckIntervalSeconds": 30,
  "healthyThresholdCount": 2,
  "unhealthyThresholdCount": 3,
  "deregistrationDelay": 30  // å¿«é€Ÿç§»é™¤ä¸å¥åº·å®¹å™¨
}
```

---

## å…­ã€æµ‹è¯•éªŒè¯

### 6.1 æœ¬åœ°æµ‹è¯•è„šæœ¬

```python
# scripts/test_cold_start.py
import time
import requests
import subprocess

def test_cold_start():
    print("ğŸ§ª Cold Start Test")
    print("="*60)

    # 1. åœæ­¢æœåŠ¡
    print("Stopping service...")
    subprocess.run(["docker", "compose", "down"])
    time.sleep(5)

    # 2. å¯åŠ¨æœåŠ¡ï¼ˆæ¨¡æ‹ŸFargateå†·å¯åŠ¨ï¼‰
    print("Starting service...")
    start_time = time.time()
    subprocess.run(["docker", "compose", "up", "-d"])

    # 3. ç­‰å¾…å¥åº·æ£€æŸ¥é€šè¿‡
    while True:
        try:
            response = requests.get("http://localhost:8000/health", timeout=2)
            if response.status_code == 200:
                startup_time = time.time() - start_time
                print(f"âœ“ Service ready in {startup_time:.2f}s")
                break
        except:
            time.sleep(1)

    # 4. å‘é€é¦–æ¬¡æŸ¥è¯¢
    print("\nSending first query...")
    query_start = time.time()
    response = requests.post(
        "http://localhost:8000/query",
        json={"query": "test query", "mode": "naive"},
        timeout=120
    )
    query_time = time.time() - query_start

    print(f"âœ“ First query completed in {query_time:.2f}s")
    print(f"ğŸ“Š Total cold start time: {startup_time + query_time:.2f}s")

    # 5. å‘é€åç»­æŸ¥è¯¢ï¼ˆéªŒè¯é¢„çƒ­æ•ˆæœï¼‰
    print("\nSending second query...")
    query_start = time.time()
    response = requests.post(
        "http://localhost:8000/query",
        json={"query": "test query 2", "mode": "naive"},
        timeout=60
    )
    query_time2 = time.time() - query_start

    print(f"âœ“ Second query completed in {query_time2:.2f}s")
    print(f"ğŸ“ˆ Speedup: {query_time / query_time2:.1f}x")

if __name__ == "__main__":
    test_cold_start()
```

### 6.2 æœŸæœ›ç»“æœ

**ä¿®æ”¹å‰**:
```
âœ“ Service ready in 10.2s
âœ“ First query completed in 62.5s
ğŸ“Š Total cold start time: 72.7s
âœ“ Second query completed in 4.3s
ğŸ“ˆ Speedup: 14.5x
```

**ä¿®æ”¹åï¼ˆç›®æ ‡ï¼‰**:
```
âœ“ Service ready in 28.5s
âœ“ First query completed in 6.2s
ğŸ“Š Total cold start time: 34.7s
âœ“ Second query completed in 4.1s
ğŸ“ˆ Speedup: 1.5x
```

---

## ä¸ƒã€æ€»ç»“

| æ–¹æ¡ˆ | å®æ–½éš¾åº¦ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ | æ¨èåº¦ |
|------|---------|------|---------|--------|
| æ–¹æ¡ˆ1: é¢„çƒ­Workers | â­ ç®€å• | â­â­â­â­â­ æœ€ä½³ | æ‰€æœ‰åœºæ™¯ | â­â­â­â­â­ |
| æ–¹æ¡ˆ2: å‡å°‘Workers | â­ ç®€å• | â­â­â­ ä¸­ç­‰ | ä½æµé‡ | â­â­â­â­ |
| æ–¹æ¡ˆ3: å¼‚æ­¥åˆå§‹åŒ– | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | éœ€APIå…¼å®¹æ€§ | â­â­â­ |
| æ–¹æ¡ˆ4: Keep-Alive | â­â­ ç®€å• | â­â­â­â­ ä¼˜ç§€ | ä¸­é«˜æµé‡ | â­â­â­â­ |
| æ–¹æ¡ˆ5: Lambda@Edge | â­â­â­â­ å¤æ‚ | â­â­â­â­ ä¼˜ç§€ | å…¨çƒåˆ†å¸ƒ | â­â­ |

**æœ€ç»ˆå»ºè®®**:
- **ç«‹å³**: å®æ–½æ–¹æ¡ˆ1+2ï¼ŒæŠ•å…¥äº§å‡ºæ¯”æœ€é«˜
- **åç»­**: æ ¹æ®æµé‡å¢é•¿ï¼Œè€ƒè™‘æ–¹æ¡ˆ4

**ROIåˆ†æ**:
- å¼€å‘æ—¶é—´: 1-2å°æ—¶
- ç”¨æˆ·ä½“éªŒæå‡: 47%
- æˆæœ¬èŠ‚çœ: 33%
- **æŠ•èµ„å›æŠ¥ç‡: éå¸¸é«˜** âœ…

---

**åˆ›å»ºæ—¶é—´**: 2025-10-23
**ä½œè€…**: Claude Code
