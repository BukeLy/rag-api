"""
æŸ¥è¯¢è·¯ç”±ï¼ˆå¤šç§Ÿæˆ· LightRAG çŸ¥è¯†å›¾è°±è®¿é—®ï¼‰
"""

import os
import re
import json
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from typing import Optional

from src.logger import logger
from src.config import config  # ä½¿ç”¨é›†ä¸­é…ç½®ç®¡ç†
from src.tenant_deps import get_tenant_id
from src.multi_tenant import get_tenant_lightrag
from .models import QueryRequest, QueryResponse

# å¯¼å…¥ LightRAG æŸ¥è¯¢å‚æ•°
try:
    from lightrag import QueryParam
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ›¿ä»£ç±»
    class QueryParam:
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)

router = APIRouter()

# ä»é…ç½®ç®¡ç†ç±»è¯»å–æŸ¥è¯¢ä¼˜åŒ–å‚æ•°
DEFAULT_TOP_K = config.lightrag_query.top_k
DEFAULT_CHUNK_TOP_K = config.lightrag_query.chunk_top_k


def strip_think_tags(text: str) -> str:
    """Remove <think>...</think> markers from LLM output."""
    if not text:
        return text
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    return re.sub(r'\n{3,}', '\n\n', text).strip()


@router.post("/query", response_model=QueryResponse)
async def query_rag(
    request: QueryRequest,
    tenant_id: str = Depends(get_tenant_id)
) -> QueryResponse:
    """
    æŸ¥è¯¢ RAG ç³»ç»Ÿï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ï¼Œç›´æ¥è®¿é—® LightRAG çŸ¥è¯†å›¾è°±ï¼‰

    **v2.0 æ–°ç‰¹æ€§**ï¼š
    - âœ¨ **å¯¹è¯å†å²æ”¯æŒ**ï¼šé€šè¿‡ `conversation_history` å®ç°å¤šè½®å¯¹è¯
    - âœ¨ **è‡ªå®šä¹‰æç¤ºè¯**ï¼šé€šè¿‡ `user_prompt` å®šåˆ¶å›ç­”é£æ ¼
    - âœ¨ **å“åº”æ ¼å¼æ§åˆ¶**ï¼šæ”¯æŒ paragraph/list/json æ ¼å¼
    - âœ¨ **å…³é”®è¯æå–**ï¼šé€šè¿‡ `hl_keywords`/`ll_keywords` ç²¾å‡†æ£€ç´¢
    - âœ¨ **Token é™åˆ¶**ï¼šç²¾ç»†æ§åˆ¶è¾“å‡ºé•¿åº¦
    - âœ¨ **è°ƒè¯•æ¨¡å¼**ï¼š`only_need_context=true` ä»…è¿”å›ä¸Šä¸‹æ–‡

    **å¤šç§Ÿæˆ·æ”¯æŒ**ï¼š
    - ğŸ”’ **ç§Ÿæˆ·éš”ç¦»**ï¼šæ¯ä¸ªç§Ÿæˆ·çš„æ•°æ®å®Œå…¨éš”ç¦»
    - ğŸ¯ **å¿…å¡«å‚æ•°**ï¼š`?tenant_id=your_tenant_id`
    - âš¡ **å…±äº«èµ„æº**ï¼šLLM/Embedding å‡½æ•°å…±äº«ï¼ŒèŠ‚çœèµ„æº

    **æŸ¥è¯¢æ¨¡å¼**ï¼š
    - `naive`: å‘é‡æ£€ç´¢ï¼ˆ**æœ€å¿«**ï¼Œæ¨èæ—¥å¸¸ä½¿ç”¨ï¼Œ15-20ç§’ï¼‰
    - `local`: å±€éƒ¨çŸ¥è¯†å›¾è°±ï¼ˆé€‚åˆç²¾ç¡®æŸ¥è¯¢ï¼‰
    - `global`: å…¨å±€çŸ¥è¯†å›¾è°±ï¼ˆå®Œæ•´ï¼Œä½†è¾ƒæ…¢ï¼‰
    - `hybrid`: æ··åˆæ¨¡å¼
    - `mix`: å…¨åŠŸèƒ½æ··åˆï¼ˆæ…¢ï¼Œä½†ç»“æœæœ€å…¨é¢ï¼‰

    **æ€§èƒ½ä¼˜åŒ–**ï¼š
    - `top_k=20`ï¼ˆä»é»˜è®¤ 60 å‡å°‘ï¼‰
    - `chunk_top_k=10`ï¼ˆä»é»˜è®¤ 20 å‡å°‘ï¼‰
    - `max_async=8`ï¼ˆä» 4 æå‡ï¼Œä¼˜åŒ–å®ä½“åˆå¹¶ï¼‰
    - `enable_rerank=True`ï¼ˆæå‡ç›¸å…³æ€§ï¼Œå¢åŠ  2-3ç§’ï¼‰
    """
    # è·å–ç§Ÿæˆ·ä¸“å±çš„ LightRAG å®ä¾‹
    lightrag = await get_tenant_lightrag(tenant_id)
    if not lightrag:
        raise HTTPException(
            status_code=503,
            detail=f"LightRAG is not ready for tenant: {tenant_id}"
        )

    try:
        # ç›´æ¥ä½¿ç”¨ LightRAG æŸ¥è¯¢ï¼ˆç»•è¿‡ RAGAnything è§£æå™¨ï¼‰
        from lightrag import QueryParam

        # æ„å»ºæŸ¥è¯¢å‚æ•°ï¼ˆåŒ…å« v2.0 æ–°å¢å‚æ•°ï¼‰
        query_param_kwargs = {
            "mode": request.mode,
            "top_k": DEFAULT_TOP_K,
            "chunk_top_k": DEFAULT_CHUNK_TOP_K,
            "enable_rerank": True,
            "only_need_context": request.only_need_context,
        }

        # æ·»åŠ é«˜çº§å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        if request.response_type:
            query_param_kwargs["response_type"] = request.response_type

        if request.hl_keywords:
            query_param_kwargs["hl_keywords"] = request.hl_keywords

        if request.ll_keywords:
            query_param_kwargs["ll_keywords"] = request.ll_keywords

        if request.max_entity_tokens:
            query_param_kwargs["max_entity_tokens"] = request.max_entity_tokens

        if request.max_relation_tokens:
            query_param_kwargs["max_relation_tokens"] = request.max_relation_tokens

        if request.max_total_tokens:
            query_param_kwargs["max_total_tokens"] = request.max_total_tokens

        if request.user_prompt:
            query_param_kwargs["user_prompt"] = request.user_prompt

        if request.conversation_history:
            query_param_kwargs["conversation_history"] = request.conversation_history

        # åˆ›å»º QueryParam
        query_param = QueryParam(**query_param_kwargs)

        # æ‰§è¡ŒæŸ¥è¯¢
        answer = await lightrag.aquery(
            request.query,
            param=query_param
        )

        # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦æˆåŠŸ
        if answer is None:
            error_msg = "Query failed: LLM API returned no response. Please check your API configuration and quota."
            logger.error(f"[Tenant {tenant_id}] {error_msg} (query: {request.query[:50]}...)")
            return {"answer": error_msg}

        # æ¸…ç† LLM è¾“å‡ºä¸­çš„ think æ ‡ç­¾
        answer = strip_think_tags(answer)

        logger.info(f"[Tenant {tenant_id}] Query successful: {request.query[:50]}... (mode: {request.mode})")
        return {"answer": answer}

    except Exception as e:
        logger.error(f"[Tenant {tenant_id}] Error during query: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/query/stream")
async def query_stream(
    request: QueryRequest,
    tenant_id: str = Depends(get_tenant_id)
):
    """
    æµå¼æŸ¥è¯¢ RAG ç³»ç»Ÿï¼ˆSSE æ ¼å¼ï¼Œå®æ—¶æ¨é€ç»“æœï¼‰

    **æ–°ç‰¹æ€§**ï¼š
    - âš¡ **å®æ—¶è¾“å‡º**ï¼šæŸ¥è¯¢ç»“æœé€æ­¥æ¨é€ï¼Œç”¨æˆ·ä½“éªŒæ›´å¥½
    - ğŸ“¡ **SSE æ ¼å¼**ï¼šæ ‡å‡† Server-Sent Events æ ¼å¼
    - ğŸ”„ **æ”¯æŒæ‰€æœ‰é«˜çº§å‚æ•°**ï¼šä¸ `/query` ç«¯ç‚¹åŠŸèƒ½ä¸€è‡´
    - ğŸ”’ **å¤šç§Ÿæˆ·éš”ç¦»**ï¼šæ¯ä¸ªç§Ÿæˆ·çš„æ•°æ®å®Œå…¨éš”ç¦»

    **ä½¿ç”¨åœºæ™¯**ï¼š
    - é•¿æ—¶é—´æŸ¥è¯¢ï¼ˆéœ€è¦å®æ—¶åé¦ˆï¼‰
    - èŠå¤©ç•Œé¢ï¼ˆé€å­—è¾“å‡ºï¼‰
    - éœ€è¦å–æ¶ˆé•¿æ—¶é—´æŸ¥è¯¢çš„åœºæ™¯

    **å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹**ï¼š
    ```javascript
    const eventSource = new EventSource('/query/stream?tenant_id=xxx');

    eventSource.onmessage = (event) => {
        const data = JSON.parse(event.data);
        if (data.done) {
            console.log('æŸ¥è¯¢å®Œæˆ');
            eventSource.close();
        } else {
            console.log('æ”¶åˆ°æ•°æ®å—:', data.chunk);
        }
    };

    eventSource.onerror = (error) => {
        console.error('è¿æ¥é”™è¯¯:', error);
        eventSource.close();
    };
    ```

    **Python å®¢æˆ·ç«¯ç¤ºä¾‹**ï¼š
    ```python
    import requests
    import json

    response = requests.post(
        "http://api.com/query/stream?tenant_id=xxx",
        json={"query": "ä»€ä¹ˆæ˜¯ AIï¼Ÿ", "mode": "naive"},
        stream=True
    )

    for line in response.iter_lines():
        if line:
            line_str = line.decode('utf-8')
            if line_str.startswith('data: '):
                data = json.loads(line_str[6:])  # å»é™¤ "data: " å‰ç¼€
                if data.get('done'):
                    break
                print(data.get('chunk'), end='', flush=True)
    ```
    """
    # è·å–ç§Ÿæˆ·ä¸“å±çš„ LightRAG å®ä¾‹
    lightrag = await get_tenant_lightrag(tenant_id)
    if not lightrag:
        raise HTTPException(
            status_code=503,
            detail=f"LightRAG is not ready for tenant: {tenant_id}"
        )

    async def generate():
        """å¼‚æ­¥ç”Ÿæˆå™¨ï¼šæµå¼è¾“å‡ºæŸ¥è¯¢ç»“æœ"""
        try:
            # æ„å»ºæŸ¥è¯¢å‚æ•°ï¼ˆä¸ /query ç«¯ç‚¹ç›¸åŒï¼‰
            from lightrag import QueryParam

            query_param_kwargs = {
                "mode": request.mode,
                "top_k": DEFAULT_TOP_K,
                "chunk_top_k": DEFAULT_CHUNK_TOP_K,
                "enable_rerank": True,
                "only_need_context": request.only_need_context,
            }

            # æ·»åŠ é«˜çº§å‚æ•°
            if request.response_type:
                query_param_kwargs["response_type"] = request.response_type
            if request.hl_keywords:
                query_param_kwargs["hl_keywords"] = request.hl_keywords
            if request.ll_keywords:
                query_param_kwargs["ll_keywords"] = request.ll_keywords
            if request.max_entity_tokens:
                query_param_kwargs["max_entity_tokens"] = request.max_entity_tokens
            if request.max_relation_tokens:
                query_param_kwargs["max_relation_tokens"] = request.max_relation_tokens
            if request.max_total_tokens:
                query_param_kwargs["max_total_tokens"] = request.max_total_tokens
            if request.user_prompt:
                query_param_kwargs["user_prompt"] = request.user_prompt
            if request.conversation_history:
                query_param_kwargs["conversation_history"] = request.conversation_history

            query_param = QueryParam(**query_param_kwargs)

            # æ£€æŸ¥ LightRAG æ˜¯å¦æ”¯æŒæµå¼æŸ¥è¯¢
            if hasattr(lightrag, 'aquery_stream'):
                # ä½¿ç”¨ LightRAG çš„åŸç”Ÿæµå¼ API
                async for chunk in lightrag.aquery_stream(request.query, param=query_param):
                    cleaned_chunk = strip_think_tags(chunk)
                    if cleaned_chunk:
                        yield f"data: {json.dumps({'chunk': cleaned_chunk, 'done': False})}\n\n"
            else:
                # Fallbackï¼šä¸€æ¬¡æ€§è·å–å…¨éƒ¨ç»“æœç„¶ååˆ†å—å‘é€
                logger.warning(f"[Tenant {tenant_id}] LightRAG does not support streaming, using fallback mode")
                answer = await lightrag.aquery(request.query, param=query_param)

                # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦æˆåŠŸ
                if answer is None:
                    error_msg = "Query failed: LLM API returned no response. Please check your API configuration and quota."
                    logger.error(f"[Tenant {tenant_id}] {error_msg} (query: {request.query[:50]}...)")
                    yield f"data: {json.dumps({'chunk': error_msg, 'done': False})}\n\n"
                else:
                    answer = strip_think_tags(answer)

                    # å°†ç»“æœåˆ†å—å‘é€ï¼ˆæ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼‰
                    chunk_size = 50  # æ¯å— 50 ä¸ªå­—ç¬¦
                    for i in range(0, len(answer), chunk_size):
                        chunk = answer[i:i + chunk_size]
                        yield f"data: {json.dumps({'chunk': chunk, 'done': False})}\n\n"

            # å‘é€å®Œæˆæ ‡è®°
            yield f"data: {json.dumps({'done': True})}\n\n"

            logger.info(f"[Tenant {tenant_id}] Stream query successful: {request.query[:50]}... (mode: {request.mode})")

        except Exception as e:
            logger.error(f"[Tenant {tenant_id}] Error during stream query: {e}", exc_info=True)
            # å‘é€é”™è¯¯ä¿¡æ¯
            error_data = {
                "error": str(e),
                "done": True
            }
            yield f"data: {json.dumps(error_data)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²
        }
    )
