"""
æ–‡æ¡£æ’å…¥è·¯ç”±ï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ï¼‰
"""

import os
import shutil
import uuid
from datetime import datetime
from pathlib import Path
from fastapi import APIRouter, HTTPException, UploadFile, File, BackgroundTasks, Query, Depends
from typing import Optional, List

from src.logger import logger
from src.rag import select_parser_by_file
from src.tenant_deps import get_tenant_id
from src.multi_tenant import get_tenant_lightrag
from .models import TaskStatus, TaskInfo
from .task_store import create_task, create_batch, get_batch, get_task, update_task

# å¯¼å…¥ RAG-Anything å¼‚å¸¸ç±»å‹
try:
    from raganything.parser import MineruExecutionError
except ImportError:
    class MineruExecutionError(Exception):
        pass

# å¯¼å…¥è¿œç¨‹ MinerU å¤„ç†ç›¸å…³æ¨¡å—
from src.file_url_service import get_file_service

router = APIRouter()


async def process_document_task(
    task_id: str,
    tenant_id: str,
    doc_id: str,
    temp_file_path: str,
    original_filename: str,
    parser: Optional[str] = "auto",
    vlm_mode: str = "off",
    deepseek_mode: Optional[str] = None
):
    """
    åå°å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆæ”¯æŒå¤šç§Ÿæˆ·éš”ç¦» + VLM æ¨¡å¼ + DeepSeek-OCRï¼‰

    Args:
        task_id: ä»»åŠ¡ID
        tenant_id: ç§Ÿæˆ·ID
        doc_id: æ–‡æ¡£ID
        temp_file_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        original_filename: åŸå§‹æ–‡ä»¶å
        parser: è§£æå™¨ç±»å‹ ("deepseek-ocr" / "mineru" / "docling" / "auto" / None)
                None è¡¨ç¤ºçº¯æ–‡æœ¬æ–‡ä»¶ï¼Œç›´æ¥æ’å…¥æ— éœ€è§£æ
        vlm_mode: VLM å¤„ç†æ¨¡å¼ï¼ˆ"off" / "selective" / "full"ï¼‰
        deepseek_mode: DeepSeek-OCR æ¨¡å¼ ("free_ocr" / "grounding" / None)
    """
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
        update_task(task_id, tenant_id, status=TaskStatus.PROCESSING, updated_at=datetime.now().isoformat())
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Started processing: {original_filename} (parser: {parser})")
        
        # è·å–ç§Ÿæˆ·ä¸“å±çš„ LightRAG å®ä¾‹
        lightrag_instance = await get_tenant_lightrag(tenant_id)
        if not lightrag_instance:
            raise Exception(f"LightRAG is not ready for tenant: {tenant_id}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ–‡æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨è½»é‡çº§ç›´æ¥æ’å…¥
        file_ext = Path(original_filename).suffix.lower()
        if file_ext in ['.txt', '.md', '.markdown']:
            logger.info(f"[Task {task_id}] Detected text file, using lightweight direct insertion")
            
            # ç›´æ¥è¯»å–æ–‡æœ¬å†…å®¹
            with open(temp_file_path, 'r', encoding='utf-8') as f:
                text_content = f.read()
            
            if not text_content or len(text_content.strip()) == 0:
                raise ValueError(f"Empty text file: {original_filename}")
            
            # ç›´æ¥æ’å…¥åˆ°ç§Ÿæˆ·çš„ LightRAG å®ä¾‹ï¼ˆè½»é‡çº§ï¼Œæ— éœ€è§£æï¼‰
            await lightrag_instance.ainsert(text_content, file_paths=original_filename)
            logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Text content inserted directly to LightRAG ({len(text_content)} characters, file: {original_filename})")
        else:
            # éæ–‡æœ¬æ–‡ä»¶ï¼Œéœ€è¦ä½¿ç”¨è§£æå™¨
            if parser is None:
                raise ValueError(f"Parser is None for non-text file: {original_filename}. This should not happen.")

            # å¤„ç† DeepSeek-OCR
            if parser == "deepseek-ocr":
                try:
                    from src.deepseek_ocr_client import create_client, DSSeekMode
                    from src.document_complexity import DocumentComplexityAnalyzer
                    from src.tenant_config import get_tenant_config_manager

                    # ğŸ†• åŠ è½½ç§Ÿæˆ·é…ç½®
                    config_manager = get_tenant_config_manager()
                    tenant_config = config_manager.get(tenant_id)
                    merged_config = config_manager.merge_with_global(tenant_config)
                    ds_ocr_config = merged_config["ds_ocr"]

                    # åˆ›å»º DeepSeek-OCR å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ç§Ÿæˆ·é…ç½®ï¼‰
                    ds_client = create_client(
                        api_key=ds_ocr_config["api_key"],
                        base_url=ds_ocr_config["base_url"],
                        model_name=ds_ocr_config["model"],
                        timeout=ds_ocr_config["timeout"],
                        max_tokens=ds_ocr_config["max_tokens"],
                        dpi=ds_ocr_config["dpi"],
                        default_mode=ds_ocr_config["default_mode"],
                        fallback_enabled=ds_ocr_config["fallback_enabled"],
                        fallback_mode=ds_ocr_config["fallback_mode"],
                        min_output_threshold=ds_ocr_config["min_output_threshold"]
                    )

                    # ç¡®å®šä½¿ç”¨çš„æ¨¡å¼
                    if deepseek_mode:
                        mode = DSSeekMode(deepseek_mode)
                    else:
                        mode = DSSeekMode.FREE_OCR  # é»˜è®¤æ¨¡å¼

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–‡è¯­è¨€æç¤ºï¼ˆç®€å•è¡¨æ ¼ <10 å­—åœºæ™¯ï¼‰
                    chinese_hint = False
                    try:
                        analyzer = DocumentComplexityAnalyzer()
                        features = analyzer.get_document_features(temp_file_path)
                        if (features.chinese_char_count > 0 and
                            features.chinese_char_count < 10):
                            chinese_hint = True
                            logger.info(f"[Task {task_id}] Chinese hint enabled (chars={features.chinese_char_count})")
                    except Exception as e:
                        logger.warning(f"[Task {task_id}] Failed to analyze Chinese chars: {e}")

                    # è°ƒç”¨ DeepSeek-OCRï¼ˆå¼‚æ­¥ï¼‰
                    markdown_text = await ds_client.parse_document(
                        file_path=temp_file_path,
                        mode=mode,
                        chinese_hint=chinese_hint
                    )

                    # ç›´æ¥æ’å…¥åˆ°ç§Ÿæˆ·çš„ LightRAG å®ä¾‹
                    await lightrag_instance.ainsert(markdown_text, file_paths=original_filename)
                    logger.info(
                        f"[Task {task_id}] [Tenant {tenant_id}] Document parsed using DeepSeek-OCR "
                        f"(mode={mode.value}, {len(markdown_text)} chars, file: {original_filename})"
                    )
                except Exception as e:
                    logger.error(f"[Task {task_id}] DeepSeek-OCR failed: {e}", exc_info=True)
                    raise

            # å¤„ç† MinerU
            elif parser == "mineru":
                mineru_mode = os.getenv("MINERU_MODE", "local")

                # æ ¹æ® MinerU æ¨¡å¼é€‰æ‹©å¤„ç†ç­–ç•¥
                if mineru_mode == "remote":
                    # ä½¿ç”¨è¿œç¨‹ MinerU å¤„ç†
                    try:
                        await process_with_remote_mineru(
                            task_id=task_id,
                            tenant_id=tenant_id,
                            file_path=temp_file_path,
                            filename=original_filename,
                            doc_id=doc_id,
                            vlm_mode=vlm_mode
                        )
                        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Document processed using remote MinerU API (vlm_mode={vlm_mode})")
                    except Exception as e:
                        logger.warning(f"[Task {task_id}] [Tenant {tenant_id}] Remote MinerU failed: {e}")
                        raise  # ä¸å†å›é€€åˆ°æœ¬åœ°å¤„ç†ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
                else:
                    # æœ¬åœ°å¤„ç†ï¼šéœ€è¦ä½¿ç”¨ RAGAnything è§£æå™¨
                    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦åˆ›å»ºä¸´æ—¶çš„ RAGAnything å®ä¾‹ï¼ˆä½¿ç”¨ç§Ÿæˆ·çš„ LightRAGï¼‰
                    from raganything import RAGAnything, RAGAnythingConfig

                    config = RAGAnythingConfig(
                        working_dir="./rag_local_storage",
                        parser="mineru",
                        enable_image_processing=True,  # ğŸ”¥ å¯ç”¨å›¾ç‰‡å¤„ç†ï¼ˆæ‰€æœ‰ parser éƒ½æ”¯æŒï¼‰
                        enable_table_processing=True,
                        enable_equation_processing=True,
                    )

                    # ğŸ†• ä» LightRAG å®ä¾‹è·å– vision_model_func
                    vision_func = getattr(lightrag_instance, 'vision_model_func', None)

                    if vision_func is None:
                        logger.warning(f"[Task {task_id}] [Tenant {tenant_id}] vision_model_func not found, image understanding disabled")

                    rag_anything = RAGAnything(
                        config=config,
                        lightrag=lightrag_instance,
                        vision_model_func=vision_func  # ğŸ†• ä¼ é€’ VLM å‡½æ•°
                    )
                    await rag_anything.process_document_complete(file_path=temp_file_path, output_dir="./output")
                    logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Document parsed using MinerU parser with VLM (mode: {mineru_mode})")

            # å¤„ç† Docling
            else:
                # Docling æˆ–å…¶ä»– parserï¼šä½¿ç”¨ RAGAnything
                from raganything import RAGAnything, RAGAnythingConfig

                config = RAGAnythingConfig(
                    working_dir="./rag_local_storage",
                    parser=parser,
                    enable_image_processing=True,
                    enable_table_processing=(parser == "docling"),
                    enable_equation_processing=False,
                )

                vision_func = getattr(lightrag_instance, 'vision_model_func', None)

                if vision_func is None:
                    logger.warning(f"[Task {task_id}] [Tenant {tenant_id}] vision_model_func not found, image understanding disabled")

                rag_anything = RAGAnything(
                    config=config,
                    lightrag=lightrag_instance,
                    vision_model_func=vision_func
                )
                await rag_anything.process_document_complete(file_path=temp_file_path, output_dir="./output")
                logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Document parsed using {parser} parser")
        
        # å¤„ç†æˆåŠŸ
        update_task(
            task_id, tenant_id,
            status=TaskStatus.COMPLETED,
            updated_at=datetime.now().isoformat(),
            result={
                "message": "Document processed successfully",
                "doc_id": doc_id,
                "filename": original_filename
            }
        )
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Completed successfully: {original_filename}")
        
    except ValueError as e:
        # éªŒè¯é”™è¯¯ï¼ˆå®¢æˆ·ç«¯é”™è¯¯ï¼‰
        update_task(
            task_id, tenant_id,
            status=TaskStatus.FAILED,
            updated_at=datetime.now().isoformat(),
            error=f"Validation error: {str(e)}"
        )
        logger.error(f"[Task {task_id}] [Tenant {tenant_id}] Validation error: {e}", exc_info=True)

    except MineruExecutionError as e:
        # MinerU è§£æé”™è¯¯
        error_msg = str(e)
        if "Unknown file suffix" in error_msg or "Unsupported" in error_msg:
            error_text = f"Unsupported file format: {original_filename}"
        else:
            error_text = f"Document parsing failed: {original_filename}"

        update_task(
            task_id, tenant_id,
            status=TaskStatus.FAILED,
            updated_at=datetime.now().isoformat(),
            error=error_text
        )
        logger.error(f"[Task {task_id}] [Tenant {tenant_id}] MinerU error: {error_msg}", exc_info=True)

    except OSError as e:
        # æ–‡ä»¶ç³»ç»Ÿé”™è¯¯
        update_task(
            task_id, tenant_id,
            status=TaskStatus.FAILED,
            updated_at=datetime.now().isoformat(),
            error="File system error occurred"
        )
        logger.error(f"[Task {task_id}] [Tenant {tenant_id}] File system error: {e}", exc_info=True)

    except Exception as e:
        # å…¶ä»–æœªé¢„æœŸçš„é”™è¯¯
        update_task(
            task_id, tenant_id,
            status=TaskStatus.FAILED,
            updated_at=datetime.now().isoformat(),
            error=f"Internal server error: {str(e)}"
        )
        logger.error(f"[Task {task_id}] [Tenant {tenant_id}] Unexpected error: {e}", exc_info=True)
        
    finally:
        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶æ€»æ˜¯è¢«åˆ é™¤
        if os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
                logger.info(f"[Task {task_id}] Cleaned up temporary file: {temp_file_path}")
            except OSError as e:
                logger.warning(f"[Task {task_id}] Failed to clean up temporary file: {e}")


@router.post("/insert", status_code=202)
async def insert_document(
    doc_id: str = Query(
        ...,
        description="æ–‡æ¡£å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºåœ¨çŸ¥è¯†å›¾è°±ä¸­æ ‡è¯†æ–‡æ¡£",
        example="research_paper_001",
        min_length=1,
        max_length=200
    ),
    file: UploadFile = File(
        ...,
        description="è¦ä¸Šä¼ çš„æ–‡æ¡£æ–‡ä»¶ï¼ˆæ”¯æŒ PDFã€DOCXã€TXTã€MDã€å›¾ç‰‡ç­‰ï¼‰"
    ),
    background_tasks: BackgroundTasks = None,
    parser: Optional[str] = Query(
        default="auto",
        description="""è§£æå™¨é€‰æ‹©ï¼š
- `auto`: æ™ºèƒ½é€‰æ‹©ï¼ˆæ¨èï¼Œæ ¹æ®æ–‡ä»¶ç±»å‹å’Œå¤§å°è‡ªåŠ¨å†³ç­–ï¼‰
- `mineru`: å¼ºå¤§çš„å¤šæ¨¡æ€è§£æå™¨ï¼ˆæ”¯æŒ OCRã€è¡¨æ ¼ã€å…¬å¼ï¼Œå†…å­˜å ç”¨å¤§ï¼‰
- `docling`: è½»é‡çº§è§£æå™¨ï¼ˆå¿«é€Ÿå¤„ç†ç®€å•æ–‡æ¡£ï¼Œå†…å­˜å ç”¨å°ï¼‰
""",
        pattern="^(auto|mineru|docling)$"
    ),
    vlm_mode: str = Query(
        default=None,
        description="""VLM å¤„ç†æ¨¡å¼ï¼ˆå¯é€‰ï¼‰ï¼š
- `off`: ä»… Markdownï¼ˆæœ€å¿«ï¼Œé»˜è®¤ï¼‰
- `selective`: æ··åˆæ¨¡å¼ï¼ˆé€‰æ‹©æ€§å¤„ç†é‡è¦å›¾è¡¨ï¼Œå¹³è¡¡æ€§èƒ½å’Œè´¨é‡ï¼‰
- `full`: å®Œæ•´ RAG-Anything å¤„ç†ï¼ˆæœ€é«˜è´¨é‡ï¼Œå¯ç”¨ä¸Šä¸‹æ–‡å¢å¼ºï¼‰
- å¦‚æœä¸æä¾›ï¼Œå°†ä½¿ç”¨ç¯å¢ƒå˜é‡ RAG_VLM_MODE çš„é»˜è®¤å€¼
""",
        pattern="^(off|selective|full)?$"
    ),
    tenant_id: str = Depends(get_tenant_id)
):
    """
    ä¸Šä¼ æ–‡æ¡£å¹¶å¼‚æ­¥å¤„ç†ï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ï¼‰

    **æµç¨‹è¯´æ˜**ï¼š
    1. ä¸Šä¼ æ–‡ä»¶ï¼Œç«‹å³è¿”å› `task_id`ï¼ˆHTTP 202 Acceptedï¼‰
    2. åå°å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆè§£æã€æå–å®ä½“ã€æ„å»ºçŸ¥è¯†å›¾è°±ï¼‰
    3. ä½¿ç”¨ `GET /task/{task_id}` æŸ¥è¯¢å¤„ç†çŠ¶æ€

    ---

    **ğŸ”’ å¤šç§Ÿæˆ·æ”¯æŒ**ï¼š
    - **ç§Ÿæˆ·éš”ç¦»**ï¼šæ¯ä¸ªç§Ÿæˆ·çš„æ–‡æ¡£å®Œå…¨éš”ç¦»ï¼Œäº’ä¸å¯è§
    - **å¿…å¡«å‚æ•°**ï¼š`?tenant_id=your_tenant_id`
    - **ç¤ºä¾‹**ï¼š`POST /insert?tenant_id=tenant_a&doc_id=doc_001`

    ---

    **ğŸ“‚ æ–‡ä»¶å¤„ç†ç­–ç•¥**ï¼š

    | æ–‡ä»¶ç±»å‹ | å¤„ç†æ–¹å¼ | æ€§èƒ½ |
    |---------|---------|-----|
    | **çº¯æ–‡æœ¬** (.txt, .md) | ç›´æ¥æ’å…¥ LightRAG | âš¡ æå¿«ï¼ˆ< 1ç§’ï¼‰ |
    | **å›¾ç‰‡** (.jpg, .png) | MinerU OCR | ğŸ¢ è¾ƒæ…¢ï¼ˆOCR å¤„ç†ï¼‰ |
    | **PDF/Office < 500KB** | Docling å¿«é€Ÿè§£æ | âš¡ å¿«é€Ÿ |
    | **PDF/Office > 500KB** | MinerU æ·±åº¦è§£æ | ğŸ¢ è¾ƒæ…¢ä½†å‡†ç¡® |

    ---

    **âš™ï¸ è§£æå™¨å‚æ•°**ï¼ˆä»…å¯¹éæ–‡æœ¬æ–‡ä»¶ç”Ÿæ•ˆï¼‰ï¼š

    - **`auto`**ï¼ˆæ¨èï¼‰ï¼šè‡ªåŠ¨é€‰æ‹©æœ€ä½³è§£æå™¨
    - **`mineru`**ï¼šå¼ºå¤§çš„å¤šæ¨¡æ€è§£æå™¨
        - âœ… æ”¯æŒ OCRï¼ˆå›¾ç‰‡ã€æ‰«æä»¶ï¼‰
        - âœ… æ”¯æŒè¡¨æ ¼æå–
        - âœ… æ”¯æŒæ•°å­¦å…¬å¼
        - âŒ å†…å­˜å ç”¨å¤§ï¼ˆå»ºè®®ä½¿ç”¨è¿œç¨‹æ¨¡å¼ï¼‰
    - **`docling`**ï¼šè½»é‡çº§è§£æå™¨
        - âœ… å¿«é€Ÿå¤„ç†ç®€å•æ–‡æ¡£
        - âœ… å†…å­˜å ç”¨å°
        - âŒ ä¸æ”¯æŒå¤æ‚å¸ƒå±€

    ---

    **ğŸ“Š æ”¯æŒçš„æ–‡ä»¶æ ¼å¼**ï¼š
    - **æ–‡æ¡£**: PDF, DOCX, DOC, TXT, MD
    - **å›¾ç‰‡**: PNG, JPG, JPEG, BMP
    - **å…¶ä»–**: æ ¹æ® RAG-Anything æ”¯æŒçš„æ ¼å¼

    ---

    **âš ï¸ æ–‡ä»¶é™åˆ¶**ï¼š
    - **æœ€å¤§æ–‡ä»¶å¤§å°**: 100 MB
    - **æ–‡ä»¶ä¸èƒ½ä¸ºç©º**ï¼ˆ0 å­—èŠ‚ï¼‰
    - **æ–‡ä»¶åå®‰å…¨æ£€æŸ¥**ï¼šè‡ªåŠ¨è¿‡æ»¤è·¯å¾„éå†æ”»å‡»

    ---

    **ğŸ“ è¿”å›ç¤ºä¾‹**ï¼š

    ```json
    {
        "message": "Document processing started",
        "task_id": "550e8400-e29b-41d4-a716-446655440000",
        "doc_id": "research_paper_001",
        "filename": "AIç ”ç©¶æŠ¥å‘Š.pdf",
        "tenant_id": "tenant_a",
        "status": "pending"
    }
    ```

    ---

    **ğŸ” åç»­æ“ä½œ**ï¼š

    ä½¿ç”¨è¿”å›çš„ `task_id` æŸ¥è¯¢å¤„ç†çŠ¶æ€ï¼š

    ```bash
    GET /task/{task_id}?tenant_id=tenant_a
    ```

    ---

    **âŒ é”™è¯¯å¤„ç†**ï¼š

    - `400 Bad Request`: ç©ºæ–‡ä»¶ã€æ–‡ä»¶è¿‡å¤§ã€ä¸æ”¯æŒçš„æ ¼å¼
    - `503 Service Unavailable`: RAG æœåŠ¡æœªå°±ç»ª
    """
    # éªŒè¯ parser å‚æ•°
    if parser not in ["mineru", "docling", "auto"]:
        raise HTTPException(status_code=400, detail=f"Invalid parser: {parser}. Must be 'mineru', 'docling', or 'auto'.")

    # è¯»å– VLM æ¨¡å¼ï¼ˆä¼˜å…ˆçº§ï¼šè¯·æ±‚å‚æ•° > ç¯å¢ƒå˜é‡ï¼‰
    effective_vlm_mode = vlm_mode if vlm_mode else os.getenv("RAG_VLM_MODE", "off")
    if effective_vlm_mode not in ["off", "selective", "full"]:
        raise HTTPException(status_code=400, detail=f"Invalid vlm_mode: {effective_vlm_mode}. Must be 'off', 'selective', or 'full'.")

    # ä¿ç•™åŸå§‹æ–‡ä»¶åï¼ˆä»…ç”¨äºæ—¥å¿—ï¼‰
    original_filename = file.filename or "unnamed_file"

    # æå–æ–‡ä»¶æ‰©å±•åï¼ˆä»…ç”¨äºæ—¥å¿—å’Œè§£æå™¨é€‰æ‹©ï¼‰
    file_extension = Path(original_filename).suffix.lower() if original_filename else ""

    # ä½¿ç”¨ UUID ç”Ÿæˆå®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶å
    safe_filename = f"{uuid.uuid4()}{file_extension}"
    temp_file_path = f"/tmp/{safe_filename}"
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
        with open(temp_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆç©ºæ–‡ä»¶æ£€æŸ¥ï¼‰
        file_size = os.path.getsize(temp_file_path)
        if file_size == 0:
            # ç«‹å³åˆ é™¤ç©ºæ–‡ä»¶
            os.remove(temp_file_path)
            raise HTTPException(status_code=400, detail=f"Empty file: {original_filename}")
        
        # é™åˆ¶æ–‡ä»¶å¤§å°ï¼ˆä¾‹å¦‚æœ€å¤§ 100MBï¼‰
        max_file_size = 100 * 1024 * 1024  # 100MB
        if file_size > max_file_size:
            os.remove(temp_file_path)
            raise HTTPException(
                status_code=400, 
                detail=f"File too large: {original_filename} ({file_size} bytes, max: {max_file_size} bytes)"
            )
        
        # æ™ºèƒ½é€‰æ‹©è§£æå™¨
        selected_parser = parser
        deepseek_mode = None  # é»˜è®¤å€¼
        if parser == "auto":
            selected_parser, deepseek_mode = select_parser_by_file(
                original_filename,
                file_size,
                file_path=temp_file_path
            )
            parser_desc = selected_parser if selected_parser else "direct_insert (text file)"
            mode_desc = f", mode={deepseek_mode}" if deepseek_mode else ""
            logger.info(f"Auto-selected parser for {original_filename} ({file_size} bytes): {parser_desc}{mode_desc}")

        # ç”Ÿæˆä»»åŠ¡ ID
        task_id = str(uuid.uuid4())
        current_time = datetime.now().isoformat()

        # åˆ›å»ºä»»åŠ¡è®°å½•ï¼ˆæŒ‰ç§Ÿæˆ·éš”ç¦»ï¼‰
        task_info = TaskInfo(
            task_id=task_id,
            status=TaskStatus.PENDING,
            doc_id=doc_id,
            filename=original_filename,
            created_at=current_time,
            updated_at=current_time
        )
        create_task(task_info, tenant_id)

        # æ·»åŠ åå°ä»»åŠ¡ï¼ˆä¼ é€’ç§Ÿæˆ·IDã€è§£æå™¨ã€VLMæ¨¡å¼ã€DS-OCRæ¨¡å¼ï¼‰
        background_tasks.add_task(
            process_document_task,
            task_id=task_id,
            tenant_id=tenant_id,
            doc_id=doc_id,
            temp_file_path=temp_file_path,
            original_filename=original_filename,
            parser=selected_parser,
            vlm_mode=effective_vlm_mode,
            deepseek_mode=deepseek_mode
        )

        parser_display = selected_parser if selected_parser else "direct_insert"
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Created task for file: {original_filename} (size: {file_size} bytes, doc_id: {doc_id}, parser: {parser_display}, vlm_mode: {effective_vlm_mode})")

        # ç«‹å³è¿”å› 202 + task_id
        return {
            "task_id": task_id,
            "tenant_id": tenant_id,
            "status": TaskStatus.PENDING,
            "message": "Document upload accepted. Processing in background.",
            "doc_id": doc_id,
            "filename": original_filename,
            "parser": parser_display,
            "vlm_mode": effective_vlm_mode,
            "file_size": file_size
        }
    
    except HTTPException:
        # ç›´æ¥é‡æ–°æŠ›å‡º HTTP å¼‚å¸¸
        raise
    
    except Exception as e:
        # å¦‚æœåˆ›å»ºä»»åŠ¡å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
            except:
                pass
        logger.error(f"Failed to create processing task: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to create task: {str(e)}")


async def process_with_remote_mineru(
    task_id: str,
    tenant_id: str,
    file_path: str,
    filename: str,
    doc_id: str,
    vlm_mode: str = "off"
):
    """
    ä½¿ç”¨è¿œç¨‹ MinerU å¤„ç†æ–‡æ¡£ï¼ˆæ”¯æŒå¤šç§Ÿæˆ· + VLM æ¨¡å¼ï¼‰

    Args:
        task_id: ä»»åŠ¡ ID
        tenant_id: ç§Ÿæˆ· ID
        file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
        filename: åŸå§‹æ–‡ä»¶å
        doc_id: æ–‡æ¡£ ID
        vlm_mode: VLM å¤„ç†æ¨¡å¼ï¼ˆ"off" / "selective" / "full"ï¼‰
    """
    try:
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Starting remote MinerU processing: {filename} (vlm_mode={vlm_mode})")

        # è·å–æ–‡ä»¶æœåŠ¡å®ä¾‹å’Œç§Ÿæˆ·çš„ LightRAG å®ä¾‹
        file_service = get_file_service()
        lightrag_instance = await get_tenant_lightrag(tenant_id)

        if not lightrag_instance:
            raise Exception(f"LightRAG instance not available for tenant: {tenant_id}")

        # è·å– VLM å‡½æ•°ï¼ˆç”¨äº selective/full æ¨¡å¼ï¼‰
        vision_func = getattr(lightrag_instance, 'vision_model_func', None)
        if vlm_mode in ["selective", "full"] and not vision_func:
            logger.warning(f"[Task {task_id}] vision_model_func not found, falling back to off mode")
            vlm_mode = "off"

        # æ³¨å†Œæ–‡ä»¶è·å– URLï¼ˆ8000 ç«¯å£ï¼‰
        file_url = await file_service.register_file(file_path, filename)
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] File registered: {file_url}")

        # ğŸ†• åŠ è½½ç§Ÿæˆ·é…ç½®
        from src.tenant_config import get_tenant_config_manager
        config_manager = get_tenant_config_manager()
        tenant_config = config_manager.get(tenant_id)
        merged_config = config_manager.merge_with_global(tenant_config)
        mineru_config = merged_config["mineru"]

        # è°ƒç”¨ MinerU å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ç§Ÿæˆ·é…ç½®ï¼‰
        from src.mineru_client import create_client, FileTask, ParseOptions
        client = create_client(
            api_token=mineru_config["api_token"],
            base_url=mineru_config["base_url"],
            timeout=mineru_config["timeout"],
            max_concurrent_requests=mineru_config["max_concurrent_requests"],
            requests_per_minute=mineru_config["requests_per_minute"],
            retry_max_attempts=mineru_config["retry_max_attempts"],
            poll_timeout=mineru_config["poll_timeout"]
        )

        # åˆ›å»ºæ–‡ä»¶ä»»åŠ¡
        file_task = FileTask(url=file_url, data_id=doc_id)

        # é…ç½®è§£æé€‰é¡¹ï¼ˆä½¿ç”¨ç§Ÿæˆ·é…ç½®ï¼‰
        options = ParseOptions(
            enable_formula=True,
            enable_table=True,
            language="ch",
            model_version=mineru_config["model_version"]
        )

        # è°ƒç”¨è¿œç¨‹ MinerU API
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Calling remote MinerU API...")
        result = await client.parse_documents([file_task], options, wait_for_completion=True)

        if result.is_completed:
            logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Remote MinerU parsing completed")

            # è¯»å– VLM é…ç½®å‚æ•°
            importance_threshold = float(os.getenv("RAG_IMPORTANCE_THRESHOLD", "0.5"))
            rag_config = {
                "context_window": int(os.getenv("RAG_CONTEXT_WINDOW", "2")),
                "context_mode": os.getenv("RAG_CONTEXT_MODE", "page"),
                "max_context_tokens": int(os.getenv("RAG_MAX_CONTEXT_TOKENS", "3000")),
            }

            # ä½¿ç”¨ç»“æœå¤„ç†å™¨å¤„ç† MinerU ç»“æœ
            from src.mineru_result_processor import get_result_processor
            processor = get_result_processor()

            # å¤„ç†ç»“æœå¹¶ç›´æ¥æ’å…¥ LightRAGï¼ˆæ”¯æŒä¸‰ç§æ¨¡å¼ï¼‰
            logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Processing MinerU result (mode={vlm_mode})...")
            process_result = await processor.process_mineru_result(
                result=result,
                lightrag_instance=lightrag_instance,
                mode=vlm_mode,
                vision_func=vision_func,
                original_filename=filename,
                importance_threshold=importance_threshold,
                rag_config=rag_config
            )

            logger.info(f"[Task {task_id}] [Tenant {tenant_id}] MinerU result processed: {process_result}")

        else:
            error_msg = result.error_message or "Unknown error"
            logger.error(f"[Task {task_id}] [Tenant {tenant_id}] Remote MinerU failed: {error_msg}")
            raise Exception(f"Remote MinerU processing failed: {error_msg}")
        
    except Exception as e:
        logger.error(f"[Task {task_id}] [Tenant {tenant_id}] Remote MinerU processing error: {e}", exc_info=True)
        # æ¸…ç†æ–‡ä»¶
        try:
            file_id = file_url.split('/')[-2] if 'file_url' in locals() else None
            if file_id:
                file_service.cleanup_file(file_id=file_id)
        except:
            pass
        raise


@router.post("/batch")
async def insert_batch(
    files: List[UploadFile] = File(...),
    doc_ids: Optional[str] = Query(None),
    parser: str = Query("auto"),
    vlm_mode: str = Query(default=None, pattern="^(off|selective|full)?$"),
    background_tasks: BackgroundTasks = None,
    tenant_id: str = Depends(get_tenant_id)
):
    """
    æ‰¹é‡æ–‡æ¡£æ’å…¥ç«¯ç‚¹ï¼ˆä¼˜åŒ–ï¼šå•æ¬¡ API è°ƒç”¨å¤„ç†å¤šä¸ªæ–‡ä»¶ï¼‰
    
    **å‚æ•°è¯´æ˜ï¼š**
    - `files`: æ–‡ä»¶åˆ—è¡¨ï¼ˆæœ€å¤š 100 ä¸ªæ–‡ä»¶ï¼‰
    - `doc_ids`: å¯é€‰çš„æ–‡æ¡£ ID åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼Œå¯¹åº” files é¡ºåºï¼‰
    - `parser`: è§£æå™¨é€‰æ‹© ('auto', 'mineru', 'docling')
    
    **åŠŸèƒ½ç‰¹æ€§ï¼š**
    - å¹¶å‘å¤„ç†å¤šä¸ªæ–‡ä»¶ï¼Œå……åˆ†åˆ©ç”¨ç³»ç»Ÿèµ„æº
    - æ”¯æŒè‡ªåŠ¨æ–‡ä»¶ç±»å‹æ£€æµ‹ä¸æœ€ä¼˜è§£æå™¨é€‰æ‹©
    - æ‰¹é‡ä»»åŠ¡ç»Ÿä¸€ç®¡ç†å’Œè¿›åº¦è·Ÿè¸ª
    - å•ä¸ªæ–‡ä»¶å¤±è´¥ä¸å½±å“å…¶ä»–æ–‡ä»¶å¤„ç†
    
    **è¿”å›å€¼ï¼š**
    ```json
    {
        "batch_id": "xxx-yyy-zzz",
        "total_files": 5,
        "tasks": [
            {
                "task_id": "task-1",
                "doc_id": "doc-1",
                "filename": "file1.pdf",
                "status": "PENDING"
            }
        ]
    }
    ```
    """
    # éªŒè¯ parser å‚æ•°
    if parser not in ["mineru", "docling", "auto"]:
        raise HTTPException(status_code=400, detail=f"Invalid parser: {parser}")

    # è¯»å– VLM æ¨¡å¼
    effective_vlm_mode = vlm_mode if vlm_mode else os.getenv("RAG_VLM_MODE", "off")
    if effective_vlm_mode not in ["off", "selective", "full"]:
        raise HTTPException(status_code=400, detail=f"Invalid vlm_mode: {effective_vlm_mode}")

    # é™åˆ¶æ–‡ä»¶æ•°é‡
    if not files or len(files) > 100:
        raise HTTPException(status_code=400, detail="File count must be between 1 and 100")
    
    # è§£æ doc_ids
    doc_ids_list = []
    if doc_ids:
        doc_ids_list = [did.strip() for did in doc_ids.split(',')]
        if len(doc_ids_list) != len(files):
            raise HTTPException(status_code=400, detail=f"doc_ids count ({len(doc_ids_list)}) must match files count ({len(files)})")
    else:
        # ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆéšæœº doc_id
        doc_ids_list = [str(uuid.uuid4()) for _ in files]
    
    # åˆ›å»ºæ‰¹é‡ä»»åŠ¡ ID
    batch_id = str(uuid.uuid4())
    tasks = []

    logger.info(f"[Batch {batch_id}] [Tenant {tenant_id}] Starting batch insert with {len(files)} files, parser: {parser}")

    try:
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for idx, (file, doc_id) in enumerate(zip(files, doc_ids_list)):
            try:
                # éªŒè¯æ–‡ä»¶å
                original_filename = file.filename or f"file_{idx}"
                
                # æå–æ–‡ä»¶æ‰©å±•åï¼ˆä»…ç”¨äºæ—¥å¿—å’Œè§£æå™¨é€‰æ‹©ï¼‰
                file_extension = Path(original_filename).suffix.lower()
                
                # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶è·¯å¾„
                safe_filename = f"{uuid.uuid4()}{file_extension}"
                temp_file_path = f"/tmp/{safe_filename}"
                
                # ä¿å­˜æ–‡ä»¶
                with open(temp_file_path, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)
                
                # éªŒè¯æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(temp_file_path)
                if file_size == 0:
                    os.remove(temp_file_path)
                    logger.warning(f"[Batch {batch_id}] Skipped empty file: {original_filename}")
                    continue
                
                max_file_size = 100 * 1024 * 1024  # 100MB
                if file_size > max_file_size:
                    os.remove(temp_file_path)
                    logger.warning(f"[Batch {batch_id}] Skipped oversized file: {original_filename}")
                    continue
                
                # æ™ºèƒ½é€‰æ‹©è§£æå™¨
                selected_parser = parser
                deepseek_mode = None  # é»˜è®¤å€¼
                if parser == "auto":
                    selected_parser, deepseek_mode = select_parser_by_file(
                        original_filename,
                        file_size,
                        file_path=temp_file_path
                    )

                parser_display = selected_parser if selected_parser else "direct_insert"

                # ç”Ÿæˆä»»åŠ¡ ID
                task_id = str(uuid.uuid4())
                current_time = datetime.now().isoformat()

                # åˆ›å»ºä»»åŠ¡è®°å½•ï¼ˆæŒ‰ç§Ÿæˆ·éš”ç¦»ï¼‰
                task_info = TaskInfo(
                    task_id=task_id,
                    status=TaskStatus.PENDING,
                    doc_id=doc_id,
                    filename=original_filename,
                    created_at=current_time,
                    updated_at=current_time
                )
                create_task(task_info, tenant_id)

                # æ·»åŠ åå°ä»»åŠ¡
                background_tasks.add_task(
                    process_document_task,
                    task_id=task_id,
                    tenant_id=tenant_id,
                    doc_id=doc_id,
                    temp_file_path=temp_file_path,
                    original_filename=original_filename,
                    parser=selected_parser,
                    vlm_mode=effective_vlm_mode,
                    deepseek_mode=deepseek_mode
                )

                logger.info(f"[Batch {batch_id}] [Tenant {tenant_id}] Created task {task_id} for file: {original_filename} (parser: {parser_display})")

                tasks.append({
                    "task_id": task_id,
                    "doc_id": doc_id,
                    "filename": original_filename,
                    "status": TaskStatus.PENDING,
                    "parser": parser_display,
                    "file_size": file_size
                })
            
            except Exception as e:
                logger.error(f"[Batch {batch_id}] Error processing file {idx}: {e}")
                continue
        
        if not tasks:
            raise HTTPException(status_code=400, detail="No valid files in batch")

        logger.info(f"[Batch {batch_id}] [Tenant {tenant_id}] Batch insert created: {len(tasks)} tasks")

        # è®°å½•æ‰¹é‡ä»»åŠ¡æ˜ å°„ï¼ˆä¿®å¤å‰ç¼€åŒ¹é…çš„bugï¼‰
        task_ids = [task["task_id"] for task in tasks]
        current_time = datetime.now().isoformat()
        create_batch(
            batch_id=batch_id,
            tenant_id=tenant_id,
            task_ids=task_ids,
            created_at=current_time
        )

        return {
            "batch_id": batch_id,
            "tenant_id": tenant_id,
            "total_files": len(files),
            "accepted_files": len(tasks),
            "message": f"Batch accepted. Processing {len(tasks)} files.",
            "tasks": tasks
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Batch {batch_id}] Failed to create batch: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to create batch: {str(e)}")


@router.get("/batch/{batch_id}")
async def get_batch_status(
    batch_id: str,
    tenant_id: str = Depends(get_tenant_id)
):
    """
    æŸ¥è¯¢æ‰¹é‡ä»»åŠ¡è¿›åº¦ï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ï¼Œä½¿ç”¨ BATCH_STOREï¼‰

    **å¤šç§Ÿæˆ·æ”¯æŒ**ï¼š
    - ğŸ”’ **ç§Ÿæˆ·éš”ç¦»**ï¼šåªèƒ½æŸ¥è¯¢æœ¬ç§Ÿæˆ·çš„æ‰¹é‡ä»»åŠ¡
    - ğŸ¯ **å¿…å¡«å‚æ•°**ï¼š`?tenant_id=your_tenant_id`

    **è¿”å›å€¼ï¼š**
    ```json
    {
        "batch_id": "xxx-yyy-zzz",
        "tenant_id": "tenant_a",
        "total_tasks": 5,
        "completed": 3,
        "failed": 1,
        "pending": 1,
        "processing": 0,
        "progress": 0.6,
        "created_at": "2025-10-30T...",
        "tasks": [
            {
                "task_id": "task-1",
                "doc_id": "doc-1",
                "filename": "file1.pdf",
                "status": "completed",
                "created_at": "...",
                "updated_at": "..."
            }
        ]
    }
    ```
    """
    logger.info(f"[Batch {batch_id}] [Tenant {tenant_id}] Querying batch status")

    # ä» BATCH_STORE è·å–æ‰¹é‡ä»»åŠ¡ä¿¡æ¯ï¼ˆä¿®å¤å‰ç¼€åŒ¹é…çš„bugï¼‰
    batch_info = get_batch(batch_id, tenant_id)

    if not batch_info:
        raise HTTPException(
            status_code=404,
            detail=f"Batch not found: {batch_id} (tenant: {tenant_id})"
        )

    # è·å–æ‰€æœ‰å…³è”çš„ä»»åŠ¡è¯¦æƒ…
    task_ids = batch_info["task_ids"]
    related_tasks = []

    for task_id in task_ids:
        task_info = get_task(task_id, tenant_id)
        if task_info:
            related_tasks.append({
                "task_id": task_id,
                "doc_id": task_info.doc_id,
                "filename": task_info.filename,
                "status": task_info.status,
                "created_at": task_info.created_at,
                "updated_at": task_info.updated_at,
                "error": task_info.error,  # åŒ…å«é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                "result": task_info.result  # åŒ…å«ç»“æœä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            })
        else:
            # ä»»åŠ¡å¯èƒ½å·²è¢«æ¸…ç†ï¼Œè®°å½•è­¦å‘Š
            logger.warning(f"[Batch {batch_id}] Task {task_id} not found in task store")
            related_tasks.append({
                "task_id": task_id,
                "doc_id": "unknown",
                "filename": "unknown",
                "status": "unknown",
                "created_at": batch_info["created_at"],
                "updated_at": batch_info["created_at"]
            })

    # ç»Ÿè®¡è¿›åº¦
    completed = sum(1 for t in related_tasks if t['status'] == TaskStatus.COMPLETED)
    failed = sum(1 for t in related_tasks if t['status'] == TaskStatus.FAILED)
    pending = sum(1 for t in related_tasks if t['status'] == TaskStatus.PENDING)
    processing = sum(1 for t in related_tasks if t['status'] == TaskStatus.PROCESSING)

    return {
        "batch_id": batch_id,
        "tenant_id": tenant_id,
        "total_tasks": batch_info["total"],
        "completed": completed,
        "failed": failed,
        "pending": pending,
        "processing": processing,
        "progress": completed / batch_info["total"] if batch_info["total"] > 0 else 0,
        "created_at": batch_info["created_at"],
        "tasks": related_tasks
    }

