"""
æ–‡æ¡£æ’å…¥è·¯ç”±ï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ï¼‰
"""

import os
import shutil
import uuid
from datetime import datetime
from pathlib import Path
from fastapi import APIRouter, HTTPException, UploadFile, File, BackgroundTasks, Query, Depends
from typing import Optional, List

from src.logger import logger
from src.rag import select_parser_by_file
from src.tenant_deps import get_tenant_id
from src.multi_tenant import get_tenant_lightrag
from .models import TaskStatus, TaskInfo
from .task_store import TASK_STORE, DOCUMENT_PROCESSING_SEMAPHORE, create_task

# å¯¼å…¥ RAG-Anything å¼‚å¸¸ç±»å‹
try:
    from raganything.parser import MineruExecutionError
except ImportError:
    class MineruExecutionError(Exception):
        pass

# å¯¼å…¥è¿œç¨‹ MinerU å¤„ç†ç›¸å…³æ¨¡å—
from src.file_url_service import get_file_service

router = APIRouter()


async def process_document_task(task_id: str, tenant_id: str, doc_id: str, temp_file_path: str, original_filename: str, parser: str = "auto"):
    """
    åå°å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆæ”¯æŒå¤šç§Ÿæˆ·éš”ç¦»ï¼‰

    Args:
        task_id: ä»»åŠ¡ID
        tenant_id: ç§Ÿæˆ·ID
        doc_id: æ–‡æ¡£ID
        temp_file_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        original_filename: åŸå§‹æ–‡ä»¶å
        parser: è§£æå™¨ç±»å‹ ("mineru" / "docling" / "auto")
    """
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
        if tenant_id in TASK_STORE and task_id in TASK_STORE[tenant_id]:
            TASK_STORE[tenant_id][task_id].status = TaskStatus.PROCESSING
            TASK_STORE[tenant_id][task_id].updated_at = datetime.now().isoformat()
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Started processing: {original_filename} (parser: {parser})")
        
        # è·å–ç§Ÿæˆ·ä¸“å±çš„ LightRAG å®ä¾‹
        lightrag_instance = await get_tenant_lightrag(tenant_id)
        if not lightrag_instance:
            raise Exception(f"LightRAG is not ready for tenant: {tenant_id}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ–‡æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨è½»é‡çº§ç›´æ¥æ’å…¥
        file_ext = Path(original_filename).suffix.lower()
        if file_ext in ['.txt', '.md', '.markdown']:
            logger.info(f"[Task {task_id}] Detected text file, using lightweight direct insertion")
            
            # ç›´æ¥è¯»å–æ–‡æœ¬å†…å®¹
            with open(temp_file_path, 'r', encoding='utf-8') as f:
                text_content = f.read()
            
            if not text_content or len(text_content.strip()) == 0:
                raise ValueError(f"Empty text file: {original_filename}")
            
            # ç›´æ¥æ’å…¥åˆ°ç§Ÿæˆ·çš„ LightRAG å®ä¾‹ï¼ˆè½»é‡çº§ï¼Œæ— éœ€è§£æï¼‰
            await lightrag_instance.ainsert(text_content)
            logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Text content inserted directly to LightRAG ({len(text_content)} characters)")
        else:
            # éæ–‡æœ¬æ–‡ä»¶ï¼Œéœ€è¦ä½¿ç”¨è§£æå™¨
            mineru_mode = os.getenv("MINERU_MODE", "local")
            
            # æ ¹æ® MinerU æ¨¡å¼é€‰æ‹©å¤„ç†ç­–ç•¥
            if mineru_mode == "remote" and parser == "mineru":
                # ä½¿ç”¨è¿œç¨‹ MinerU å¤„ç†
                try:
                    await process_with_remote_mineru(task_id, tenant_id, temp_file_path,
                                                   original_filename, doc_id)
                    logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Document processed using remote MinerU API")
                except Exception as e:
                    logger.warning(f"[Task {task_id}] [Tenant {tenant_id}] Remote MinerU failed: {e}")
                    raise  # ä¸å†å›é€€åˆ°æœ¬åœ°å¤„ç†ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
            else:
                # æœ¬åœ°å¤„ç†ï¼šéœ€è¦ä½¿ç”¨ RAGAnything è§£æå™¨
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦åˆ›å»ºä¸´æ—¶çš„ RAGAnything å®ä¾‹ï¼ˆä½¿ç”¨ç§Ÿæˆ·çš„ LightRAGï¼‰
                from raganything import RAGAnything, RAGAnythingConfig

                config = RAGAnythingConfig(
                    working_dir="./rag_local_storage",
                    parser=parser,
                    enable_image_processing=(parser == "mineru"),
                    enable_table_processing=(parser == "mineru"),
                    enable_equation_processing=(parser == "mineru"),
                )
                rag_anything = RAGAnything(config=config, lightrag=lightrag_instance)
                await rag_anything.process_document_complete(file_path=temp_file_path, output_dir="./output")
                logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Document parsed using {parser} parser (mode: {mineru_mode})")
        
        # å¤„ç†æˆåŠŸ
        if tenant_id in TASK_STORE and task_id in TASK_STORE[tenant_id]:
            TASK_STORE[tenant_id][task_id].status = TaskStatus.COMPLETED
            TASK_STORE[tenant_id][task_id].updated_at = datetime.now().isoformat()
            TASK_STORE[tenant_id][task_id].result = {
                "message": "Document processed successfully",
                "doc_id": doc_id,
                "filename": original_filename
            }
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Completed successfully: {original_filename}")
        
    except ValueError as e:
        # éªŒè¯é”™è¯¯ï¼ˆå®¢æˆ·ç«¯é”™è¯¯ï¼‰
        if tenant_id in TASK_STORE and task_id in TASK_STORE[tenant_id]:
            TASK_STORE[tenant_id][task_id].status = TaskStatus.FAILED
            TASK_STORE[tenant_id][task_id].updated_at = datetime.now().isoformat()
            TASK_STORE[tenant_id][task_id].error = f"Validation error: {str(e)}"
        logger.error(f"[Task {task_id}] [Tenant {tenant_id}] Validation error: {e}", exc_info=True)

    except MineruExecutionError as e:
        # MinerU è§£æé”™è¯¯
        error_msg = str(e)
        if tenant_id in TASK_STORE and task_id in TASK_STORE[tenant_id]:
            TASK_STORE[tenant_id][task_id].status = TaskStatus.FAILED
            TASK_STORE[tenant_id][task_id].updated_at = datetime.now().isoformat()

            if "Unknown file suffix" in error_msg or "Unsupported" in error_msg:
                TASK_STORE[tenant_id][task_id].error = f"Unsupported file format: {original_filename}"
            else:
                TASK_STORE[tenant_id][task_id].error = f"Document parsing failed: {original_filename}"

        logger.error(f"[Task {task_id}] [Tenant {tenant_id}] MinerU error: {error_msg}", exc_info=True)

    except OSError as e:
        # æ–‡ä»¶ç³»ç»Ÿé”™è¯¯
        if tenant_id in TASK_STORE and task_id in TASK_STORE[tenant_id]:
            TASK_STORE[tenant_id][task_id].status = TaskStatus.FAILED
            TASK_STORE[tenant_id][task_id].updated_at = datetime.now().isoformat()
            TASK_STORE[tenant_id][task_id].error = "File system error occurred"
        logger.error(f"[Task {task_id}] [Tenant {tenant_id}] File system error: {e}", exc_info=True)

    except Exception as e:
        # å…¶ä»–æœªé¢„æœŸçš„é”™è¯¯
        if tenant_id in TASK_STORE and task_id in TASK_STORE[tenant_id]:
            TASK_STORE[tenant_id][task_id].status = TaskStatus.FAILED
            TASK_STORE[tenant_id][task_id].updated_at = datetime.now().isoformat()
            TASK_STORE[tenant_id][task_id].error = f"Internal server error: {str(e)}"
        logger.error(f"[Task {task_id}] [Tenant {tenant_id}] Unexpected error: {e}", exc_info=True)
        
    finally:
        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶æ€»æ˜¯è¢«åˆ é™¤
        if os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
                logger.info(f"[Task {task_id}] Cleaned up temporary file: {temp_file_path}")
            except OSError as e:
                logger.warning(f"[Task {task_id}] Failed to clean up temporary file: {e}")


@router.post("/insert", status_code=202)
async def insert_document(
    doc_id: str,
    file: UploadFile = File(...),
    background_tasks: BackgroundTasks = None,
    parser: Optional[str] = Query(
        default="auto",
        description="è§£æå™¨é€‰æ‹©: 'mineru'(å¼ºå¤§å¤šæ¨¡æ€), 'docling'(å¿«é€Ÿè½»é‡), 'auto'(æ™ºèƒ½é€‰æ‹©)"
    ),
    tenant_id: str = Depends(get_tenant_id)
):
    """
    ä¸Šä¼ æ–‡ä»¶å¹¶å¼‚æ­¥å¤„ç†ï¼ˆæ”¯æŒå¤šç§Ÿæˆ·éš”ç¦»ï¼‰ã€‚ç«‹å³è¿”å› task_idï¼Œå®¢æˆ·ç«¯å¯é€šè¿‡ /task/{task_id} æŸ¥è¯¢å¤„ç†çŠ¶æ€ã€‚

    **å¤šç§Ÿæˆ·æ”¯æŒ**ï¼š
    - ğŸ”’ **ç§Ÿæˆ·éš”ç¦»**ï¼šæ–‡æ¡£ä»…å¯¹æŒ‡å®šç§Ÿæˆ·å¯è§
    - ğŸ¯ **å¿…å¡«å‚æ•°**ï¼š`?tenant_id=your_tenant_id`

    **æ–‡ä»¶ç±»å‹å¤„ç†ç­–ç•¥ï¼š**
    - **çº¯æ–‡æœ¬ (.txt, .md)**: ç›´æ¥æ’å…¥ LightRAGï¼Œè½»é‡å¿«é€Ÿï¼Œæ— éœ€è§£æå™¨
    - **å›¾ç‰‡ (.jpg, .png)**: ä½¿ç”¨ MinerUï¼ˆOCR èƒ½åŠ›å¼ºï¼‰
    - **PDF/Office å°æ–‡ä»¶ (< 500KB)**: ä½¿ç”¨ Doclingï¼ˆå¿«é€Ÿï¼‰
    - **PDF/Office å¤§æ–‡ä»¶ (> 500KB)**: ä½¿ç”¨ MinerUï¼ˆå¼ºå¤§å¤šæ¨¡æ€ï¼‰

    **è§£æå™¨å‚æ•°ï¼ˆä»…å¯¹éæ–‡æœ¬æ–‡ä»¶ç”Ÿæ•ˆï¼‰ï¼š**
    - `auto`: è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰
    - `mineru`: å¼ºå¤§çš„å¤šæ¨¡æ€è§£æå™¨ï¼ˆå†…å­˜å ç”¨å¤§ï¼‰
    - `docling`: è½»é‡çº§è§£æå™¨ï¼ˆå†…å­˜å ç”¨å°ï¼‰

    è¿”å› 202 Accepted è¡¨ç¤ºä»»åŠ¡å·²æ¥å—ï¼Œæ­£åœ¨å¤„ç†ä¸­ã€‚
    """
    # éªŒè¯ parser å‚æ•°
    if parser not in ["mineru", "docling", "auto"]:
        raise HTTPException(status_code=400, detail=f"Invalid parser: {parser}. Must be 'mineru', 'docling', or 'auto'.")
    
    # ä¿ç•™åŸå§‹æ–‡ä»¶åï¼ˆä»…ç”¨äºæ—¥å¿—ï¼‰
    original_filename = file.filename or "unnamed_file"
    
    # å®‰å…¨åœ°æå–æ–‡ä»¶æ‰©å±•å
    if original_filename:
        basename = os.path.basename(original_filename)
        file_extension = Path(basename).suffix.lower()
        if file_extension and not file_extension[1:].replace('_', '').replace('-', '').isalnum():
            file_extension = ""
    else:
        file_extension = ""
    
    # ä½¿ç”¨ UUID ç”Ÿæˆå®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶å
    safe_filename = f"{uuid.uuid4()}{file_extension}"
    temp_file_path = f"/tmp/{safe_filename}"
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
        with open(temp_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆç©ºæ–‡ä»¶æ£€æŸ¥ï¼‰
        file_size = os.path.getsize(temp_file_path)
        if file_size == 0:
            # ç«‹å³åˆ é™¤ç©ºæ–‡ä»¶
            os.remove(temp_file_path)
            raise HTTPException(status_code=400, detail=f"Empty file: {original_filename}")
        
        # é™åˆ¶æ–‡ä»¶å¤§å°ï¼ˆä¾‹å¦‚æœ€å¤§ 100MBï¼‰
        max_file_size = 100 * 1024 * 1024  # 100MB
        if file_size > max_file_size:
            os.remove(temp_file_path)
            raise HTTPException(
                status_code=400, 
                detail=f"File too large: {original_filename} ({file_size} bytes, max: {max_file_size} bytes)"
            )
        
        # æ™ºèƒ½é€‰æ‹©è§£æå™¨
        selected_parser = parser
        if parser == "auto":
            selected_parser = select_parser_by_file(original_filename, file_size)
            logger.info(f"Auto-selected parser for {original_filename} ({file_size} bytes): {selected_parser}")
        
        # ç”Ÿæˆä»»åŠ¡ ID
        task_id = str(uuid.uuid4())
        current_time = datetime.now().isoformat()
        
        # åˆ›å»ºä»»åŠ¡è®°å½•ï¼ˆæŒ‰ç§Ÿæˆ·éš”ç¦»ï¼‰
        task_info = TaskInfo(
            task_id=task_id,
            status=TaskStatus.PENDING,
            doc_id=doc_id,
            filename=original_filename,
            created_at=current_time,
            updated_at=current_time
        )
        create_task(task_info, tenant_id)

        # æ·»åŠ åå°ä»»åŠ¡ï¼ˆä¼ é€’ç§Ÿæˆ·IDå’Œé€‰æ‹©çš„è§£æå™¨ï¼‰
        background_tasks.add_task(
            process_document_task,
            task_id=task_id,
            tenant_id=tenant_id,  # æ–°å¢ç§Ÿæˆ·ID
            doc_id=doc_id,
            temp_file_path=temp_file_path,
            original_filename=original_filename,
            parser=selected_parser
        )

        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Created task for file: {original_filename} (size: {file_size} bytes, doc_id: {doc_id}, parser: {selected_parser})")
        
        # ç«‹å³è¿”å› 202 + task_id
        return {
            "task_id": task_id,
            "tenant_id": tenant_id,
            "status": TaskStatus.PENDING,
            "message": "Document upload accepted. Processing in background.",
            "doc_id": doc_id,
            "filename": original_filename,
            "parser": selected_parser,
            "file_size": file_size
        }
    
    except HTTPException:
        # ç›´æ¥é‡æ–°æŠ›å‡º HTTP å¼‚å¸¸
        raise
    
    except Exception as e:
        # å¦‚æœåˆ›å»ºä»»åŠ¡å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
            except:
                pass
        logger.error(f"Failed to create processing task: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to create task: {str(e)}")


async def process_with_remote_mineru(task_id: str, tenant_id: str, file_path: str,
                                   filename: str, doc_id: str):
    """
    ä½¿ç”¨è¿œç¨‹ MinerU å¤„ç†æ–‡æ¡£ï¼ˆæ”¯æŒå¤šç§Ÿæˆ·ï¼‰

    Args:
        task_id: ä»»åŠ¡ ID
        tenant_id: ç§Ÿæˆ· ID
        file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
        filename: åŸå§‹æ–‡ä»¶å
        doc_id: æ–‡æ¡£ ID
    """
    try:
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Starting remote MinerU processing: {filename}")

        # è·å–æ–‡ä»¶æœåŠ¡å®ä¾‹å’Œç§Ÿæˆ·çš„ LightRAG å®ä¾‹
        file_service = get_file_service()
        lightrag_instance = await get_tenant_lightrag(tenant_id)

        if not lightrag_instance:
            raise Exception(f"LightRAG instance not available for tenant: {tenant_id}")
        
        # æ³¨å†Œæ–‡ä»¶è·å– URLï¼ˆ8000 ç«¯å£ï¼‰
        file_url = await file_service.register_file(file_path, filename)
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] File registered: {file_url}")
        
        # è°ƒç”¨ MinerU å®¢æˆ·ç«¯
        from src.mineru_client import create_client, FileTask, ParseOptions
        client = create_client()
        
        # åˆ›å»ºæ–‡ä»¶ä»»åŠ¡
        file_task = FileTask(url=file_url, data_id=doc_id)
        
        # é…ç½®è§£æé€‰é¡¹
        options = ParseOptions(
            enable_formula=True,
            enable_table=True,
            language="ch",
            model_version=os.getenv("MINERU_MODEL_VERSION", "vlm")
        )
        
        # è°ƒç”¨è¿œç¨‹ MinerU API
        logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Calling remote MinerU API...")
        result = await client.parse_documents([file_task], options, wait_for_completion=True)

        if result.is_completed:
            logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Remote MinerU parsing completed")

            # ä¼˜åŒ–ï¼šä½¿ç”¨ç»“æœå¤„ç†å™¨ç›´æ¥å¤„ç† MinerU çš„ Markdown ç»“æœ
            from src.mineru_result_processor import get_result_processor
            processor = get_result_processor()

            # å¤„ç†ç»“æœå¹¶ç›´æ¥æ’å…¥ LightRAG
            logger.info(f"[Task {task_id}] [Tenant {tenant_id}] Processing MinerU result and inserting to LightRAG...")
            process_result = await processor.process_mineru_result(result, lightrag_instance)

            logger.info(f"[Task {task_id}] [Tenant {tenant_id}] MinerU result processed: {process_result}")

        else:
            error_msg = result.error_message or "Unknown error"
            logger.error(f"[Task {task_id}] [Tenant {tenant_id}] Remote MinerU failed: {error_msg}")
            raise Exception(f"Remote MinerU processing failed: {error_msg}")
        
    except Exception as e:
        logger.error(f"[Task {task_id}] [Tenant {tenant_id}] Remote MinerU processing error: {e}", exc_info=True)
        # æ¸…ç†æ–‡ä»¶
        try:
            file_id = file_url.split('/')[-2] if 'file_url' in locals() else None
            if file_id:
                file_service.cleanup_file(file_id=file_id)
        except:
            pass
        raise


@router.post("/batch")
async def insert_batch(
    files: List[UploadFile] = File(...),
    doc_ids: Optional[str] = Query(None),
    parser: str = Query("auto"),
    background_tasks: BackgroundTasks = None,
    tenant_id: str = Depends(get_tenant_id)
):
    """
    æ‰¹é‡æ–‡æ¡£æ’å…¥ç«¯ç‚¹ï¼ˆä¼˜åŒ–ï¼šå•æ¬¡ API è°ƒç”¨å¤„ç†å¤šä¸ªæ–‡ä»¶ï¼‰
    
    **å‚æ•°è¯´æ˜ï¼š**
    - `files`: æ–‡ä»¶åˆ—è¡¨ï¼ˆæœ€å¤š 100 ä¸ªæ–‡ä»¶ï¼‰
    - `doc_ids`: å¯é€‰çš„æ–‡æ¡£ ID åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼Œå¯¹åº” files é¡ºåºï¼‰
    - `parser`: è§£æå™¨é€‰æ‹© ('auto', 'mineru', 'docling')
    
    **åŠŸèƒ½ç‰¹æ€§ï¼š**
    - å¹¶å‘å¤„ç†å¤šä¸ªæ–‡ä»¶ï¼Œå……åˆ†åˆ©ç”¨ç³»ç»Ÿèµ„æº
    - æ”¯æŒè‡ªåŠ¨æ–‡ä»¶ç±»å‹æ£€æµ‹ä¸æœ€ä¼˜è§£æå™¨é€‰æ‹©
    - æ‰¹é‡ä»»åŠ¡ç»Ÿä¸€ç®¡ç†å’Œè¿›åº¦è·Ÿè¸ª
    - å•ä¸ªæ–‡ä»¶å¤±è´¥ä¸å½±å“å…¶ä»–æ–‡ä»¶å¤„ç†
    
    **è¿”å›å€¼ï¼š**
    ```json
    {
        "batch_id": "xxx-yyy-zzz",
        "total_files": 5,
        "tasks": [
            {
                "task_id": "task-1",
                "doc_id": "doc-1",
                "filename": "file1.pdf",
                "status": "PENDING"
            }
        ]
    }
    ```
    """
    # éªŒè¯ parser å‚æ•°
    if parser not in ["mineru", "docling", "auto"]:
        raise HTTPException(status_code=400, detail=f"Invalid parser: {parser}")
    
    # é™åˆ¶æ–‡ä»¶æ•°é‡
    if not files or len(files) > 100:
        raise HTTPException(status_code=400, detail="File count must be between 1 and 100")
    
    # è§£æ doc_ids
    doc_ids_list = []
    if doc_ids:
        doc_ids_list = [did.strip() for did in doc_ids.split(',')]
        if len(doc_ids_list) != len(files):
            raise HTTPException(status_code=400, detail=f"doc_ids count ({len(doc_ids_list)}) must match files count ({len(files)})")
    else:
        # ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆéšæœº doc_id
        doc_ids_list = [str(uuid.uuid4()) for _ in files]
    
    # åˆ›å»ºæ‰¹é‡ä»»åŠ¡ ID
    batch_id = str(uuid.uuid4())
    tasks = []

    logger.info(f"[Batch {batch_id}] [Tenant {tenant_id}] Starting batch insert with {len(files)} files, parser: {parser}")

    try:
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for idx, (file, doc_id) in enumerate(zip(files, doc_ids_list)):
            try:
                # éªŒè¯æ–‡ä»¶å
                original_filename = file.filename or f"file_{idx}"
                
                # å®‰å…¨åœ°æå–æ–‡ä»¶æ‰©å±•å
                basename = os.path.basename(original_filename)
                file_extension = Path(basename).suffix.lower()
                if file_extension and not file_extension[1:].replace('_', '').replace('-', '').isalnum():
                    file_extension = ""
                
                # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶è·¯å¾„
                safe_filename = f"{uuid.uuid4()}{file_extension}"
                temp_file_path = f"/tmp/{safe_filename}"
                
                # ä¿å­˜æ–‡ä»¶
                with open(temp_file_path, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)
                
                # éªŒè¯æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(temp_file_path)
                if file_size == 0:
                    os.remove(temp_file_path)
                    logger.warning(f"[Batch {batch_id}] Skipped empty file: {original_filename}")
                    continue
                
                max_file_size = 100 * 1024 * 1024  # 100MB
                if file_size > max_file_size:
                    os.remove(temp_file_path)
                    logger.warning(f"[Batch {batch_id}] Skipped oversized file: {original_filename}")
                    continue
                
                # æ™ºèƒ½é€‰æ‹©è§£æå™¨
                selected_parser = parser
                if parser == "auto":
                    selected_parser = select_parser_by_file(original_filename, file_size)
                
                # ç”Ÿæˆä»»åŠ¡ ID
                task_id = str(uuid.uuid4())
                current_time = datetime.now().isoformat()
                
                # åˆ›å»ºä»»åŠ¡è®°å½•ï¼ˆæŒ‰ç§Ÿæˆ·éš”ç¦»ï¼‰
                task_info = TaskInfo(
                    task_id=task_id,
                    status=TaskStatus.PENDING,
                    doc_id=doc_id,
                    filename=original_filename,
                    created_at=current_time,
                    updated_at=current_time
                )
                create_task(task_info, tenant_id)

                # æ·»åŠ åå°ä»»åŠ¡
                background_tasks.add_task(
                    process_document_task,
                    task_id=task_id,
                    tenant_id=tenant_id,  # æ–°å¢ç§Ÿæˆ·ID
                    doc_id=doc_id,
                    temp_file_path=temp_file_path,
                    original_filename=original_filename,
                    parser=selected_parser
                )

                logger.info(f"[Batch {batch_id}] [Tenant {tenant_id}] Created task {task_id} for file: {original_filename}")
                
                tasks.append({
                    "task_id": task_id,
                    "doc_id": doc_id,
                    "filename": original_filename,
                    "status": TaskStatus.PENDING,
                    "parser": selected_parser,
                    "file_size": file_size
                })
            
            except Exception as e:
                logger.error(f"[Batch {batch_id}] Error processing file {idx}: {e}")
                continue
        
        if not tasks:
            raise HTTPException(status_code=400, detail="No valid files in batch")

        logger.info(f"[Batch {batch_id}] [Tenant {tenant_id}] Batch insert created: {len(tasks)} tasks")

        return {
            "batch_id": batch_id,
            "tenant_id": tenant_id,
            "total_files": len(files),
            "accepted_files": len(tasks),
            "message": f"Batch accepted. Processing {len(tasks)} files.",
            "tasks": tasks
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Batch {batch_id}] Failed to create batch: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to create batch: {str(e)}")


@router.get("/batch/{batch_id}")
async def get_batch_status(batch_id: str):
    """
    æŸ¥è¯¢æ‰¹é‡ä»»åŠ¡è¿›åº¦
    
    **è¿”å›å€¼ï¼š**
    ```json
    {
        "batch_id": "xxx-yyy-zzz",
        "total_tasks": 5,
        "completed": 3,
        "failed": 1,
        "pending": 1,
        "progress": 0.6,
        "tasks": [...]
    }
    ```
    """
    # æ³¨æ„ï¼šéœ€è¦æœ‰æ•ˆçš„ batch_id è¿½è¸ªæœºåˆ¶
    # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æœ‰ BATCH_STORE æ¥è¿½è¸ªæ‰¹é‡ä»»åŠ¡
    logger.info(f"Querying batch status: {batch_id}")
    
    # æœç´¢æ‰€æœ‰ä»»åŠ¡ä¸­ä¸æ­¤ batch ç›¸å…³çš„ä»»åŠ¡
    # è¿™å¯ä»¥é€šè¿‡ä»»åŠ¡åç§°å‰ç¼€æˆ–å…¶ä»–æ–¹å¼å®ç°
    related_tasks = []
    
    for task_id, task_info in TASK_STORE.items():
        # ç®€å•çš„å®ç°ï¼šå¦‚æœ task_id åŒ¹é…æŸä¸ªæ¨¡å¼
        if task_id.startswith(batch_id[:8]):  # ç®€åŒ–åŒ¹é…
            related_tasks.append({
                "task_id": task_id,
                "doc_id": task_info.doc_id,
                "filename": task_info.filename,
                "status": task_info.status,
                "created_at": task_info.created_at,
                "updated_at": task_info.updated_at
            })
    
    if not related_tasks:
        raise HTTPException(status_code=404, detail=f"Batch not found: {batch_id}")
    
    # ç»Ÿè®¡è¿›åº¦
    completed = sum(1 for t in related_tasks if t['status'] == TaskStatus.COMPLETED)
    failed = sum(1 for t in related_tasks if t['status'] == TaskStatus.FAILED)
    pending = sum(1 for t in related_tasks if t['status'] == TaskStatus.PENDING)
    
    return {
        "batch_id": batch_id,
        "total_tasks": len(related_tasks),
        "completed": completed,
        "failed": failed,
        "pending": pending,
        "processing": len(related_tasks) - completed - failed - pending,
        "progress": completed / len(related_tasks) if related_tasks else 0,
        "tasks": related_tasks
    }

