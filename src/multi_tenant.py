"""
å¤šç§Ÿæˆ· LightRAG å®ä¾‹ç®¡ç†å™¨

æ”¯æŒåŸºäº workspace çš„ç§Ÿæˆ·éš”ç¦»ï¼Œä½¿ç”¨ LRU ç¼“å­˜ç®¡ç†å®ä¾‹æ± ã€‚
"""

import os
from functools import lru_cache
from typing import Dict, Optional
from contextlib import asynccontextmanager
from lightrag import LightRAG
from lightrag.utils import EmbeddingFunc
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from src.logger import logger
from src.config import config  # ä½¿ç”¨é›†ä¸­é…ç½®ç®¡ç†
from src.rate_limiter import get_rate_limiter  # å¯¼å…¥é€Ÿç‡é™åˆ¶å™¨

# æ¨¡å‹è°ƒç”¨ Future è¶…æ—¶ï¼ˆç§’ï¼‰= rate limiter ç­‰å¾… + API è°ƒç”¨ + ç¼“å†²
# ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤ 90 ç§’
MODEL_CALL_TIMEOUT = float(os.getenv("MODEL_CALL_TIMEOUT", "90"))


class MultiTenantRAGManager:
    """
    å¤šç§Ÿæˆ· RAG å®ä¾‹ç®¡ç†å™¨

    ç‰¹æ€§ï¼š
    - åŸºäº workspace çš„ç§Ÿæˆ·éš”ç¦»
    - LRU ç¼“å­˜ç®¡ç†å®ä¾‹æ± ï¼ˆæœ€å¤šç¼“å­˜ N ä¸ªç§Ÿæˆ·ï¼‰
    - å…±äº« LLM/Embedding å‡½æ•°
    - è‡ªåŠ¨æ¸…ç†ä¸æ´»è·ƒç§Ÿæˆ·å®ä¾‹
    """

    def __init__(
        self,
        max_instances: int = 50,  # æœ€å¤šç¼“å­˜ 50 ä¸ªç§Ÿæˆ·å®ä¾‹
        default_system_prompt: str = "You are a helpful assistant. Provide direct answers without showing your reasoning process.",
    ):
        self.max_instances = max_instances
        self.default_system_prompt = default_system_prompt

        # ç§Ÿæˆ·å®ä¾‹ç¼“å­˜ï¼štenant_id -> LightRAG
        self._instances: Dict[str, LightRAG] = {}

        # å…±äº«é…ç½®ï¼ˆä»é›†ä¸­é…ç½®ç®¡ç†è¯»å–ï¼‰
        self.ark_api_key = config.llm.api_key
        self.ark_base_url = config.llm.base_url
        self.ark_model = config.llm.model

        self.sf_api_key = config.embedding.api_key
        self.sf_base_url = config.embedding.base_url
        self.sf_embedding_model = config.embedding.model

        self.rerank_model = config.rerank.model

        # æ€§èƒ½é…ç½®
        self.top_k = config.lightrag_query.top_k
        self.chunk_top_k = config.lightrag_query.chunk_top_k
        self.max_async = config.llm.max_async
        self.vlm_timeout = config.llm.vlm_timeout

        # å­˜å‚¨é…ç½®
        self.use_external_storage = config.storage.use_external
        self.kv_storage = config.storage.kv_storage
        self.vector_storage = config.storage.vector_storage
        self.graph_storage = config.storage.graph_storage

        logger.info(f"MultiTenantRAGManager initialized (max_instances={max_instances})")

    def _create_llm_func(self, llm_config: Dict):
        """åˆ›å»º LLM å‡½æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·é…ç½®è¦†ç›– + é€Ÿç‡é™åˆ¶ï¼‰

        Tenant Configuration Scope:
        - âœ… Can configure: api_key, model, base_url, RateLimiter params (max_async, RPM, TPM)
        - âŒ Cannot configure: LightRAG's llm_model_max_async (always uses RateLimiter's value)

        Returns:
            tuple: (llm_func, actual_max_concurrent) - å‡½æ•°å’Œå®é™…å¹¶å‘æ•°
        """
        import asyncio

        # ä»é…ç½®ä¸­æå–å‚æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·è¦†ç›–ï¼‰
        model = llm_config.get("model", self.ark_model)
        api_key = llm_config.get("api_key", self.ark_api_key)
        base_url = llm_config.get("base_url", self.ark_base_url)

        # è·å– RateLimiter å‚æ•°ï¼ˆç§Ÿæˆ·å¯é…ç½®ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œçš„ max_async æ˜¯ RateLimiter çš„å¹¶å‘æ§åˆ¶ï¼Œä¸æ˜¯ LightRAG çš„
        requests_per_minute = llm_config.get("requests_per_minute", config.llm.requests_per_minute)
        tokens_per_minute = llm_config.get("tokens_per_minute", config.llm.tokens_per_minute)
        max_concurrent = llm_config.get("max_async", None)  # RateLimiter çš„å¹¶å‘æ•°ï¼ˆå¯é€‰ï¼‰

        # åˆ›å»ºé€Ÿç‡é™åˆ¶å™¨ï¼ˆä¼šè‡ªåŠ¨è®¡ç®— max_concurrentï¼Œé™¤éæ˜¾å¼æä¾›ï¼‰
        rate_limiter = get_rate_limiter(
            service="llm",
            max_concurrent=max_concurrent,  # ç§Ÿæˆ·çš„ RateLimiter é…ç½®
            requests_per_minute=requests_per_minute,
            tokens_per_minute=tokens_per_minute
        )

        # è·å– rate_limiter å®é™…ä½¿ç”¨çš„å¹¶å‘æ•°ï¼ˆå°†ç”¨äº LightRAGï¼‰
        actual_max_concurrent = rate_limiter.max_concurrent

        def llm_model_func(prompt, **kwargs):
            # ä¼°ç®— tokensï¼ˆç®€å•ä¼°ç®—ï¼šå­—ç¬¦æ•° / 3ï¼‰
            estimated_tokens = len(prompt) // 3 + 500  # è¾“å…¥ + é¢„ä¼°è¾“å‡º

            # åœ¨åŒæ­¥å‡½æ•°ä¸­è¿è¡Œå¼‚æ­¥é€Ÿç‡é™åˆ¶
            async def _call_with_rate_limit():
                # ğŸ”’ CRITICAL: Must acquire semaphore first to limit concurrency
                async with rate_limiter.semaphore:
                    # Then acquire rate limit permission
                    await rate_limiter.rate_limiter.acquire(estimated_tokens)

                    # Finally call the API
                    kwargs['enable_cot'] = False
                    if 'system_prompt' not in kwargs:
                        kwargs['system_prompt'] = self.default_system_prompt
                    return openai_complete_if_cache(
                        model, prompt,
                        api_key=api_key,
                        base_url=base_url,
                    )

            # å¦‚æœå·²åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œä½¿ç”¨ create_task
            # å¤„ç†åŒæ­¥/å¼‚æ­¥è°ƒç”¨ - ä¿®å¤æ­»é”é—®é¢˜
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨é¿å… asyncio.run_coroutine_threadsafe çš„æ­»é”
            import concurrent.futures

            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(asyncio.run, _call_with_rate_limit())
                # è¶…æ—¶ = 60s (rate limiteræœ€å¤§ç­‰å¾…) + 30s (APIè°ƒç”¨+ç¼“å†²)
                return future.result(timeout=MODEL_CALL_TIMEOUT)

        return llm_model_func, actual_max_concurrent

    def _create_embedding_func(self, embedding_config: Dict):
        """åˆ›å»º Embedding å‡½æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·é…ç½®è¦†ç›– + é€Ÿç‡é™åˆ¶ï¼‰"""
        import asyncio

        # ä»é…ç½®ä¸­æå–å‚æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·è¦†ç›–ï¼‰
        model = embedding_config.get("model", self.sf_embedding_model)
        api_key = embedding_config.get("api_key", self.sf_api_key)
        base_url = embedding_config.get("base_url", self.sf_base_url)
        embedding_dim = embedding_config.get("dim", config.embedding.dim)

        # è·å–é€Ÿç‡é™åˆ¶å™¨
        requests_per_minute = embedding_config.get("requests_per_minute", config.embedding.requests_per_minute)
        tokens_per_minute = embedding_config.get("tokens_per_minute", config.embedding.tokens_per_minute)
        max_concurrent = embedding_config.get("max_async", config.embedding.max_async)

        rate_limiter = get_rate_limiter(
            service="embedding",
            max_concurrent=max_concurrent,
            requests_per_minute=requests_per_minute,
            tokens_per_minute=tokens_per_minute
        )

        # è·å– rate_limiter å®é™…ä½¿ç”¨çš„å¹¶å‘æ•°ï¼ˆå°†ç”¨äº LightRAGï¼‰
        actual_max_concurrent = rate_limiter.max_concurrent

        def embedding_func_with_rate_limit(texts):
            # ä¼°ç®— tokensï¼ˆæ‰€æœ‰æ–‡æœ¬çš„æ€»å­—ç¬¦æ•° / 3ï¼‰
            total_chars = sum(len(text) for text in texts)
            estimated_tokens = total_chars // 3

            async def _call_with_rate_limit():
                # ğŸ”’ CRITICAL: Must acquire semaphore first to limit concurrency
                async with rate_limiter.semaphore:
                    # Then acquire rate limit permission
                    await rate_limiter.rate_limiter.acquire(estimated_tokens)

                    # Finally call the API
                    return openai_embed(
                        texts,
                        model=model,
                        api_key=api_key,
                        base_url=base_url
                    )

            # å¤„ç†åŒæ­¥/å¼‚æ­¥è°ƒç”¨ - ä¿®å¤æ­»é”é—®é¢˜
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨é¿å… asyncio.run_coroutine_threadsafe çš„æ­»é”
            import concurrent.futures

            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(asyncio.run, _call_with_rate_limit())
                # è¶…æ—¶ = 60s (rate limiteræœ€å¤§ç­‰å¾…) + 30s (APIè°ƒç”¨+ç¼“å†²)
                return future.result(timeout=MODEL_CALL_TIMEOUT)

        return EmbeddingFunc(
            embedding_dim=embedding_dim,
            func=embedding_func_with_rate_limit,
        ), actual_max_concurrent

    def _create_rerank_func(self, rerank_config: Dict):
        """åˆ›å»º Rerank å‡½æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·é…ç½®è¦†ç›– + é€Ÿç‡é™åˆ¶ï¼‰"""
        import asyncio

        # ä»é…ç½®ä¸­æå–å‚æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·è¦†ç›–ï¼‰
        model = rerank_config.get("model", self.rerank_model)
        api_key = rerank_config.get("api_key", self.sf_api_key)
        base_url = rerank_config.get("base_url", self.sf_base_url)

        if not model:
            return None

        try:
            from lightrag.rerank import cohere_rerank

            # è·å–é€Ÿç‡é™åˆ¶å™¨
            requests_per_minute = rerank_config.get("requests_per_minute", config.rerank.requests_per_minute)
            tokens_per_minute = rerank_config.get("tokens_per_minute", config.rerank.tokens_per_minute)
            max_concurrent = rerank_config.get("max_async", config.rerank.max_async)

            rate_limiter = get_rate_limiter(
                service="rerank",
                max_concurrent=max_concurrent,
                requests_per_minute=requests_per_minute,
                tokens_per_minute=tokens_per_minute
            )

            def rerank_func_with_rate_limit(query, documents, top_n=None, **kwargs):
                # æ¥å— **kwargs ä»¥å…¼å®¹ LightRAG å¯èƒ½ä¼ é€’çš„å…¶ä»–å‚æ•°
                # ä¼°ç®— tokensï¼ˆæŸ¥è¯¢ + æ‰€æœ‰æ–‡æ¡£çš„å­—ç¬¦æ•° / 3ï¼‰
                total_chars = len(query) + sum(len(doc) for doc in documents)
                estimated_tokens = total_chars // 3

                async def _call_with_rate_limit():
                    # ğŸ”’ CRITICAL: Must acquire semaphore first to limit concurrency
                    async with rate_limiter.semaphore:
                        # Then acquire rate limit permission
                        await rate_limiter.rate_limiter.acquire(estimated_tokens)

                        # Finally call the API
                        return cohere_rerank(
                            query=query,
                            documents=documents,
                            top_n=top_n,
                            model=model,
                            api_key=api_key,
                            base_url=f"{base_url}/rerank"
                        )

                # å¤„ç†åŒæ­¥/å¼‚æ­¥è°ƒç”¨ - ä¿®å¤æ­»é”é—®é¢˜
                # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨é¿å… asyncio.run_coroutine_threadsafe çš„æ­»é”
                import concurrent.futures

                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, _call_with_rate_limit())
                    # è¶…æ—¶ = 60s (rate limiteræœ€å¤§ç­‰å¾…) + 30s (APIè°ƒç”¨+ç¼“å†²)
                    return future.result(timeout=MODEL_CALL_TIMEOUT)

            return rerank_func_with_rate_limit

        except ImportError:
            logger.warning("lightrag.rerank not available")
            return None

    def _create_vision_model_func(self, llm_config: Dict):
        """åˆ›å»º Vision Model å‡½æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·é…ç½®è¦†ç›– + é€Ÿç‡é™åˆ¶ï¼‰"""
        import aiohttp

        # ä»é…ç½®ä¸­æå–å‚æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·è¦†ç›–ï¼‰
        model = llm_config.get("model", self.ark_model)
        api_key = llm_config.get("api_key", self.ark_api_key)
        base_url = llm_config.get("base_url", self.ark_base_url)
        vlm_timeout = llm_config.get("vlm_timeout", self.vlm_timeout)

        # è·å–é€Ÿç‡é™åˆ¶å™¨ï¼ˆVLM ä½¿ç”¨ LLM çš„é™åˆ¶ï¼‰
        requests_per_minute = llm_config.get("requests_per_minute", config.llm.requests_per_minute)
        tokens_per_minute = llm_config.get("tokens_per_minute", config.llm.tokens_per_minute)
        max_concurrent = llm_config.get("max_async", config.llm.max_async)

        rate_limiter = get_rate_limiter(
            service="llm",  # VLM å…±äº« LLM çš„é€Ÿç‡é™åˆ¶
            max_concurrent=max_concurrent,
            requests_per_minute=requests_per_minute,
            tokens_per_minute=tokens_per_minute
        )

        async def seed_vision_model_func(prompt: str, image_data: str, system_prompt: str) -> str:
            """
            ä½¿ç”¨ VLM ç†è§£å›¾ç‰‡å†…å®¹ï¼ˆå¸¦é€Ÿç‡é™åˆ¶ï¼‰

            Args:
                prompt: ä¸»è¦æç¤ºè¯ï¼ˆå¦‚"è¯·æè¿°è¿™å¼ å›¾ç‰‡"ï¼‰
                image_data: base64 ç¼–ç çš„å›¾ç‰‡æ•°æ®
                system_prompt: ç³»ç»Ÿæç¤ºè¯

            Returns:
                str: å›¾ç‰‡æè¿°æ–‡æœ¬
            """
            # ä¼°ç®— tokensï¼ˆæç¤ºè¯ + å›¾ç‰‡çº¦ 200 tokens + è¾“å‡º 500ï¼‰
            estimated_tokens = len(prompt) // 3 + 200 + 500

            # ğŸ”’ CRITICAL: Must acquire semaphore first to limit concurrency
            async with rate_limiter.semaphore:
                # Then acquire rate limit permission
                await rate_limiter.rate_limiter.acquire(estimated_tokens)

                payload = {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/png;base64,{image_data}"}
                                }
                            ]
                        }
                    ],
                    "max_tokens": 500,
                    "temperature": 0.1
                }

                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }

                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            f"{base_url}/chat/completions",
                            json=payload,
                            headers=headers,
                            timeout=aiohttp.ClientTimeout(total=vlm_timeout)
                        ) as response:
                            if response.status != 200:
                                error_text = await response.text()
                                logger.error(f"VLM API error ({response.status}): {error_text}")
                                raise Exception(f"VLM API error: {error_text}")

                            result = await response.json()
                            content = result["choices"][0]["message"]["content"]
                            logger.debug(f"VLM response: {content[:100]}...")
                            return content
                except Exception as e:
                    logger.error(f"Failed to call VLM API: {e}")
                    raise

        return seed_vision_model_func

    async def get_instance(self, tenant_id: str) -> LightRAG:
        """
        è·å–æŒ‡å®šç§Ÿæˆ·çš„ LightRAG å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰

        Args:
            tenant_id: ç§Ÿæˆ· IDï¼ˆä½œä¸º workspaceï¼‰

        Returns:
            LightRAG: è¯¥ç§Ÿæˆ·çš„ LightRAG å®ä¾‹
        """
        # éªŒè¯ tenant_id
        if not tenant_id or not isinstance(tenant_id, str):
            raise ValueError(f"Invalid tenant_id: {tenant_id}")

        # æ£€æŸ¥ç¼“å­˜
        if tenant_id in self._instances:
            logger.debug(f"Reusing cached instance for tenant: {tenant_id}")
            return self._instances[tenant_id]

        # æ£€æŸ¥å®ä¾‹æ•°é‡é™åˆ¶
        if len(self._instances) >= self.max_instances:
            # æ¸…ç†æœ€æ—§çš„å®ä¾‹ï¼ˆç®€å• FIFO ç­–ç•¥ï¼‰
            oldest_tenant = next(iter(self._instances))
            logger.info(f"Instance pool full, removing oldest tenant: {oldest_tenant}")
            del self._instances[oldest_tenant]

        # åˆ›å»ºæ–°å®ä¾‹
        logger.info(f"Creating new LightRAG instance for tenant: {tenant_id}")
        instance = await self._create_instance(tenant_id)
        self._instances[tenant_id] = instance

        return instance

    async def _create_instance(self, tenant_id: str) -> LightRAG:
        """
        ä¸ºæŒ‡å®šç§Ÿæˆ·åˆ›å»ºæ–°çš„ LightRAG å®ä¾‹

        Args:
            tenant_id: ç§Ÿæˆ· ID

        Returns:
            LightRAG: æ–°åˆ›å»ºçš„å®ä¾‹
        """
        # ğŸ†• åŠ è½½ç§Ÿæˆ·é…ç½®å¹¶ä¸å…¨å±€é…ç½®åˆå¹¶
        from src.tenant_config import get_tenant_config_manager
        config_manager = get_tenant_config_manager()
        tenant_config = config_manager.get(tenant_id)
        merged_config = config_manager.merge_with_global(tenant_config)

        # è®°å½•é…ç½®æ¥æº
        if tenant_config:
            logger.info(f"[{tenant_id}] Using tenant-specific config")
        else:
            logger.debug(f"[{tenant_id}] Using global config (no tenant config found)")

        # å‡†å¤‡ç§Ÿæˆ·ä¸“å±å‡½æ•°ï¼ˆä½¿ç”¨åˆå¹¶åçš„é…ç½®ï¼‰
        llm_func, llm_max_concurrent = self._create_llm_func(merged_config["llm"])
        embedding_func, embedding_max_concurrent = self._create_embedding_func(merged_config["embedding"])
        rerank_func = self._create_rerank_func(merged_config["rerank"])
        vision_func = self._create_vision_model_func(merged_config["llm"])  # ğŸ†• åˆ›å»º VLM å‡½æ•°

        # å‡†å¤‡å­˜å‚¨é…ç½®
        storage_kwargs = {}
        if self.use_external_storage:
            # ç›´æ¥ä¼ é€’é…ç½®å€¼ï¼Œæ”¯æŒæ‰€æœ‰å­˜å‚¨ç±»å‹ï¼ˆRedis, Qdrant, Memgraph, PostgreSQL, Neo4j ç­‰ï¼‰
            storage_kwargs["kv_storage"] = self.kv_storage
            storage_kwargs["vector_storage"] = self.vector_storage
            storage_kwargs["graph_storage"] = self.graph_storage
            storage_kwargs["doc_status_storage"] = "RedisDocStatusStorage"  # ğŸ†• ä½¿ç”¨ Redis å­˜å‚¨ doc_status
            logger.info(f"[{tenant_id}] Using external storage: KV={self.kv_storage}, Vector={self.vector_storage}, Graph={self.graph_storage}, DocStatus=RedisDocStatusStorage")

        # åˆ›å»º LightRAG å®ä¾‹
        # CRITICAL: llm_model_max_async & embedding_func_max_async MUST match RateLimiter's concurrent value
        # This ensures LightRAG's worker pool doesn't bypass rate limiting
        instance = LightRAG(
            working_dir="./rag_local_storage",  # å…±äº«å·¥ä½œç›®å½•
            workspace=tenant_id,  # å…³é”®ï¼šä½¿ç”¨ tenant_id ä½œä¸º workspace
            llm_model_func=llm_func,
            embedding_func=embedding_func,
            llm_model_max_async=llm_max_concurrent,  # ğŸ”’ Force use RateLimiter's value (not tenant-controllable)
            embedding_func_max_async=embedding_max_concurrent,  # ğŸ”’ Force use RateLimiter's value
            **storage_kwargs
        )

        logger.info(
            f"[{tenant_id}] LightRAG instance created: "
            f"LLM workers={llm_max_concurrent}, Embedding workers={embedding_max_concurrent} "
            f"(enforced by RateLimiter, tenant cannot override)"
        )

        # åˆå§‹åŒ–å­˜å‚¨
        await instance.initialize_storages()

        # åˆå§‹åŒ– Pipeline Statusï¼ˆå¤šç§Ÿæˆ·æ¨¡å¼å¿…éœ€ï¼‰
        from lightrag.kg.shared_storage import initialize_pipeline_status
        await initialize_pipeline_status()

        # é…ç½® Rerankï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if rerank_func:
            instance.rerank_model_func = rerank_func

        # ğŸ†• é™„åŠ  Vision Model å‡½æ•°ï¼ˆä¾› RAG-Anything ä½¿ç”¨ï¼‰
        instance.vision_model_func = vision_func

        logger.info(f"âœ“ LightRAG instance created for tenant: {tenant_id} (workspace={tenant_id}, VLM enabled)")
        return instance

    def remove_instance(self, tenant_id: str) -> bool:
        """
        æ‰‹åŠ¨ç§»é™¤æŒ‡å®šç§Ÿæˆ·çš„å®ä¾‹ï¼ˆé‡Šæ”¾å†…å­˜ï¼‰

        Args:
            tenant_id: ç§Ÿæˆ· ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸç§»é™¤
        """
        if tenant_id in self._instances:
            del self._instances[tenant_id]
            logger.info(f"Removed instance for tenant: {tenant_id}")
            return True
        return False

    def get_stats(self) -> dict:
        """è·å–å®ä¾‹æ± ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_instances": len(self._instances),
            "max_instances": self.max_instances,
            "tenants": list(self._instances.keys())
        }


# å…¨å±€å•ä¾‹ç®¡ç†å™¨
_manager: Optional[MultiTenantRAGManager] = None


def get_multi_tenant_manager() -> MultiTenantRAGManager:
    """è·å–å…¨å±€å¤šç§Ÿæˆ·ç®¡ç†å™¨ï¼ˆå•ä¾‹ï¼‰"""
    global _manager
    if _manager is None:
        _manager = MultiTenantRAGManager()
    return _manager


async def get_tenant_lightrag(tenant_id: str) -> LightRAG:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–æŒ‡å®šç§Ÿæˆ·çš„ LightRAG å®ä¾‹

    Args:
        tenant_id: ç§Ÿæˆ· ID

    Returns:
        LightRAG: è¯¥ç§Ÿæˆ·çš„å®ä¾‹
    """
    manager = get_multi_tenant_manager()
    return await manager.get_instance(tenant_id)