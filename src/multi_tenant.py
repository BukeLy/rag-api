"""
å¤šç§Ÿæˆ· LightRAG å®ä¾‹ç®¡ç†å™¨

æ”¯æŒåŸºäº workspace çš„ç§Ÿæˆ·éš”ç¦»ï¼Œä½¿ç”¨ LRU ç¼“å­˜ç®¡ç†å®ä¾‹æ± ã€‚
"""

import os
from functools import lru_cache
from typing import Dict, Optional
from contextlib import asynccontextmanager
from lightrag import LightRAG
from lightrag.utils import EmbeddingFunc
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from src.logger import logger
from src.config import config  # ä½¿ç”¨é›†ä¸­é…ç½®ç®¡ç†


class MultiTenantRAGManager:
    """
    å¤šç§Ÿæˆ· RAG å®ä¾‹ç®¡ç†å™¨

    ç‰¹æ€§ï¼š
    - åŸºäº workspace çš„ç§Ÿæˆ·éš”ç¦»
    - LRU ç¼“å­˜ç®¡ç†å®ä¾‹æ± ï¼ˆæœ€å¤šç¼“å­˜ N ä¸ªç§Ÿæˆ·ï¼‰
    - å…±äº« LLM/Embedding å‡½æ•°
    - è‡ªåŠ¨æ¸…ç†ä¸æ´»è·ƒç§Ÿæˆ·å®ä¾‹
    """

    def __init__(
        self,
        max_instances: int = 50,  # æœ€å¤šç¼“å­˜ 50 ä¸ªç§Ÿæˆ·å®ä¾‹
        default_system_prompt: str = "You are a helpful assistant. Provide direct answers without showing your reasoning process.",
    ):
        self.max_instances = max_instances
        self.default_system_prompt = default_system_prompt

        # ç§Ÿæˆ·å®ä¾‹ç¼“å­˜ï¼štenant_id -> LightRAG
        self._instances: Dict[str, LightRAG] = {}

        # å…±äº«é…ç½®ï¼ˆä»é›†ä¸­é…ç½®ç®¡ç†è¯»å–ï¼‰
        self.ark_api_key = config.llm.api_key
        self.ark_base_url = config.llm.base_url
        self.ark_model = config.llm.model

        self.sf_api_key = config.embedding.api_key
        self.sf_base_url = config.embedding.base_url
        self.sf_embedding_model = config.embedding.model

        self.rerank_model = config.rerank.model

        # æ€§èƒ½é…ç½®
        self.top_k = config.lightrag_query.top_k
        self.chunk_top_k = config.lightrag_query.chunk_top_k
        self.max_async = config.llm.max_async
        self.vlm_timeout = config.llm.vlm_timeout

        # å­˜å‚¨é…ç½®
        self.use_external_storage = config.storage.use_external
        self.kv_storage = config.storage.kv_storage
        self.vector_storage = config.storage.vector_storage
        self.graph_storage = config.storage.graph_storage

        logger.info(f"MultiTenantRAGManager initialized (max_instances={max_instances})")

    def _create_llm_func(self, llm_config: Dict):
        """åˆ›å»º LLM å‡½æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·é…ç½®è¦†ç›–ï¼‰"""
        # ä»é…ç½®ä¸­æå–å‚æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·è¦†ç›–ï¼‰
        model = llm_config.get("model", self.ark_model)
        api_key = llm_config.get("api_key", self.ark_api_key)
        base_url = llm_config.get("base_url", self.ark_base_url)

        def llm_model_func(prompt, **kwargs):
            kwargs['enable_cot'] = False
            if 'system_prompt' not in kwargs:
                kwargs['system_prompt'] = self.default_system_prompt
            return openai_complete_if_cache(
                model, prompt,
                api_key=api_key,
                base_url=base_url,
                **kwargs
            )
        return llm_model_func

    def _create_embedding_func(self, embedding_config: Dict):
        """åˆ›å»º Embedding å‡½æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·é…ç½®è¦†ç›–ï¼‰"""
        # ä»é…ç½®ä¸­æå–å‚æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·è¦†ç›–ï¼‰
        model = embedding_config.get("model", self.sf_embedding_model)
        api_key = embedding_config.get("api_key", self.sf_api_key)
        base_url = embedding_config.get("base_url", self.sf_base_url)
        embedding_dim = embedding_config.get("dim", config.embedding.dim)

        return EmbeddingFunc(
            embedding_dim=embedding_dim,
            func=lambda texts: openai_embed(
                texts,
                model=model,
                api_key=api_key,
                base_url=base_url
            ),
        )

    def _create_rerank_func(self, rerank_config: Dict):
        """åˆ›å»º Rerank å‡½æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·é…ç½®è¦†ç›–ï¼‰"""
        # ä»é…ç½®ä¸­æå–å‚æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·è¦†ç›–ï¼‰
        model = rerank_config.get("model", self.rerank_model)
        api_key = rerank_config.get("api_key", self.sf_api_key)
        base_url = rerank_config.get("base_url", self.sf_base_url)

        if not model:
            return None

        try:
            from lightrag.rerank import cohere_rerank
            from functools import partial

            return partial(
                cohere_rerank,
                model=model,
                api_key=api_key,
                base_url=f"{base_url}/rerank"
            )
        except ImportError:
            logger.warning("lightrag.rerank not available")
            return None

    def _create_vision_model_func(self, llm_config: Dict):
        """åˆ›å»º Vision Model å‡½æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·é…ç½®è¦†ç›–ï¼‰"""
        import aiohttp

        # ä»é…ç½®ä¸­æå–å‚æ•°ï¼ˆæ”¯æŒç§Ÿæˆ·è¦†ç›–ï¼‰
        model = llm_config.get("model", self.ark_model)
        api_key = llm_config.get("api_key", self.ark_api_key)
        base_url = llm_config.get("base_url", self.ark_base_url)
        vlm_timeout = llm_config.get("vlm_timeout", self.vlm_timeout)

        async def seed_vision_model_func(prompt: str, image_data: str, system_prompt: str) -> str:
            """
            ä½¿ç”¨ VLM ç†è§£å›¾ç‰‡å†…å®¹

            Args:
                prompt: ä¸»è¦æç¤ºè¯ï¼ˆå¦‚"è¯·æè¿°è¿™å¼ å›¾ç‰‡"ï¼‰
                image_data: base64 ç¼–ç çš„å›¾ç‰‡æ•°æ®
                system_prompt: ç³»ç»Ÿæç¤ºè¯

            Returns:
                str: å›¾ç‰‡æè¿°æ–‡æœ¬
            """
            payload = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/png;base64,{image_data}"}
                            }
                        ]
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.1
            }

            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{base_url}/chat/completions",
                        json=payload,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=vlm_timeout)
                    ) as response:
                        if response.status != 200:
                            error_text = await response.text()
                            logger.error(f"VLM API error ({response.status}): {error_text}")
                            raise Exception(f"VLM API error: {error_text}")

                        result = await response.json()
                        content = result["choices"][0]["message"]["content"]
                        logger.debug(f"VLM response: {content[:100]}...")
                        return content
            except Exception as e:
                logger.error(f"Failed to call VLM API: {e}")
                raise

        return seed_vision_model_func

    async def get_instance(self, tenant_id: str) -> LightRAG:
        """
        è·å–æŒ‡å®šç§Ÿæˆ·çš„ LightRAG å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰

        Args:
            tenant_id: ç§Ÿæˆ· IDï¼ˆä½œä¸º workspaceï¼‰

        Returns:
            LightRAG: è¯¥ç§Ÿæˆ·çš„ LightRAG å®ä¾‹
        """
        # éªŒè¯ tenant_id
        if not tenant_id or not isinstance(tenant_id, str):
            raise ValueError(f"Invalid tenant_id: {tenant_id}")

        # æ£€æŸ¥ç¼“å­˜
        if tenant_id in self._instances:
            logger.debug(f"Reusing cached instance for tenant: {tenant_id}")
            return self._instances[tenant_id]

        # æ£€æŸ¥å®ä¾‹æ•°é‡é™åˆ¶
        if len(self._instances) >= self.max_instances:
            # æ¸…ç†æœ€æ—§çš„å®ä¾‹ï¼ˆç®€å• FIFO ç­–ç•¥ï¼‰
            oldest_tenant = next(iter(self._instances))
            logger.info(f"Instance pool full, removing oldest tenant: {oldest_tenant}")
            del self._instances[oldest_tenant]

        # åˆ›å»ºæ–°å®ä¾‹
        logger.info(f"Creating new LightRAG instance for tenant: {tenant_id}")
        instance = await self._create_instance(tenant_id)
        self._instances[tenant_id] = instance

        return instance

    async def _create_instance(self, tenant_id: str) -> LightRAG:
        """
        ä¸ºæŒ‡å®šç§Ÿæˆ·åˆ›å»ºæ–°çš„ LightRAG å®ä¾‹

        Args:
            tenant_id: ç§Ÿæˆ· ID

        Returns:
            LightRAG: æ–°åˆ›å»ºçš„å®ä¾‹
        """
        # ğŸ†• åŠ è½½ç§Ÿæˆ·é…ç½®å¹¶ä¸å…¨å±€é…ç½®åˆå¹¶
        from src.tenant_config import get_tenant_config_manager
        config_manager = get_tenant_config_manager()
        tenant_config = config_manager.get(tenant_id)
        merged_config = config_manager.merge_with_global(tenant_config)

        # è®°å½•é…ç½®æ¥æº
        if tenant_config:
            logger.info(f"[{tenant_id}] Using tenant-specific config")
        else:
            logger.debug(f"[{tenant_id}] Using global config (no tenant config found)")

        # å‡†å¤‡ç§Ÿæˆ·ä¸“å±å‡½æ•°ï¼ˆä½¿ç”¨åˆå¹¶åçš„é…ç½®ï¼‰
        llm_func = self._create_llm_func(merged_config["llm"])
        embedding_func = self._create_embedding_func(merged_config["embedding"])
        rerank_func = self._create_rerank_func(merged_config["rerank"])
        vision_func = self._create_vision_model_func(merged_config["llm"])  # ğŸ†• åˆ›å»º VLM å‡½æ•°

        # å‡†å¤‡å­˜å‚¨é…ç½®
        storage_kwargs = {}
        if self.use_external_storage:
            # ç›´æ¥ä¼ é€’é…ç½®å€¼ï¼Œæ”¯æŒæ‰€æœ‰å­˜å‚¨ç±»å‹ï¼ˆRedis, Qdrant, Memgraph, PostgreSQL, Neo4j ç­‰ï¼‰
            storage_kwargs["kv_storage"] = self.kv_storage
            storage_kwargs["vector_storage"] = self.vector_storage
            storage_kwargs["graph_storage"] = self.graph_storage
            logger.info(f"[{tenant_id}] Using external storage: KV={self.kv_storage}, Vector={self.vector_storage}, Graph={self.graph_storage}")

        # åˆ›å»º LightRAG å®ä¾‹ï¼ˆä½¿ç”¨ workspace éš”ç¦»ï¼‰
        instance = LightRAG(
            working_dir="./rag_local_storage",  # å…±äº«å·¥ä½œç›®å½•
            workspace=tenant_id,  # å…³é”®ï¼šä½¿ç”¨ tenant_id ä½œä¸º workspace
            llm_model_func=llm_func,
            embedding_func=embedding_func,
            llm_model_max_async=self.max_async,
            **storage_kwargs
        )

        # åˆå§‹åŒ–å­˜å‚¨
        await instance.initialize_storages()

        # åˆå§‹åŒ– Pipeline Statusï¼ˆå¤šç§Ÿæˆ·æ¨¡å¼å¿…éœ€ï¼‰
        from lightrag.kg.shared_storage import initialize_pipeline_status
        await initialize_pipeline_status()

        # é…ç½® Rerankï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if rerank_func:
            instance.rerank_model_func = rerank_func

        # ğŸ†• é™„åŠ  Vision Model å‡½æ•°ï¼ˆä¾› RAG-Anything ä½¿ç”¨ï¼‰
        instance.vision_model_func = vision_func

        logger.info(f"âœ“ LightRAG instance created for tenant: {tenant_id} (workspace={tenant_id}, VLM enabled)")
        return instance

    def remove_instance(self, tenant_id: str) -> bool:
        """
        æ‰‹åŠ¨ç§»é™¤æŒ‡å®šç§Ÿæˆ·çš„å®ä¾‹ï¼ˆé‡Šæ”¾å†…å­˜ï¼‰

        Args:
            tenant_id: ç§Ÿæˆ· ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸç§»é™¤
        """
        if tenant_id in self._instances:
            del self._instances[tenant_id]
            logger.info(f"Removed instance for tenant: {tenant_id}")
            return True
        return False

    def get_stats(self) -> dict:
        """è·å–å®ä¾‹æ± ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_instances": len(self._instances),
            "max_instances": self.max_instances,
            "tenants": list(self._instances.keys())
        }


# å…¨å±€å•ä¾‹ç®¡ç†å™¨
_manager: Optional[MultiTenantRAGManager] = None


def get_multi_tenant_manager() -> MultiTenantRAGManager:
    """è·å–å…¨å±€å¤šç§Ÿæˆ·ç®¡ç†å™¨ï¼ˆå•ä¾‹ï¼‰"""
    global _manager
    if _manager is None:
        _manager = MultiTenantRAGManager()
    return _manager


async def get_tenant_lightrag(tenant_id: str) -> LightRAG:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–æŒ‡å®šç§Ÿæˆ·çš„ LightRAG å®ä¾‹

    Args:
        tenant_id: ç§Ÿæˆ· ID

    Returns:
        LightRAG: è¯¥ç§Ÿæˆ·çš„å®ä¾‹
    """
    manager = get_multi_tenant_manager()
    return await manager.get_instance(tenant_id)
