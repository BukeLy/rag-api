import os
from dotenv import load_dotenv
from contextlib import asynccontextmanager
from functools import partial

from src.logger import logger

# -- ä» raganything_example.py æŠ„è¿‡æ¥çš„ç»„ä»¶ --
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status
from raganything import RAGAnything, RAGAnythingConfig

# å¯¼å…¥ rerank å‡½æ•°
try:
    from lightrag.rerank import cohere_rerank
except ImportError:
    cohere_rerank = None
    logger.warning("lightrag.rerank not available, rerankåŠŸèƒ½å°†è¢«ç¦ç”¨")

# --- é…ç½® ---
load_dotenv()

# Seed 1.6 model returns <think> tags by default, breaking API responses
DEFAULT_SYSTEM_PROMPT = "You are a helpful assistant. Provide direct answers without showing your reasoning process."

# EC2 t3.small has 2 vCPUs. 4x oversubscription for I/O-bound LLM API calls.
# Empirically tested: 8 gives best throughput without hitting rate limits.
DEFAULT_MAX_ASYNC = 8

# --- å…¨å±€å®ä¾‹ï¼ˆå•ä¸€ LightRAG æ¶æ„ï¼‰---
global_lightrag_instance = None  # å•ä¸€å…±äº«çš„ LightRAG å®ä¾‹ï¼ˆæ ¸å¿ƒçŸ¥è¯†å›¾è°±ï¼‰
rag_instance_mineru = None  # MinerU: å¼ºå¤§å¤šæ¨¡æ€è§£æå™¨ï¼Œå…±äº« LightRAG
rag_instance_docling = None  # Docling: è½»é‡å¿«é€Ÿè§£æå™¨ï¼Œå…±äº« LightRAG
rag_instance = None  # é»˜è®¤å®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰

# --- RAG å®ä¾‹ç®¡ç† ---
@asynccontextmanager
async def lifespan(app):
    # å¯åŠ¨æ—¶åˆ›å»ºå•ä¸€ LightRAG å®ä¾‹ + å¤šè§£æå™¨æ¶æ„
    global global_lightrag_instance, rag_instance, rag_instance_mineru, rag_instance_docling
    logger.info("Starting up: Single LightRAG + Multiple Parsers architecture...")

    # è¯»å– LLM å’Œ Embedding é…ç½®
    ark_api_key = os.getenv("ARK_API_KEY")
    ark_base_url = os.getenv("ARK_BASE_URL")
    ark_model = os.getenv("ARK_MODEL", "seed-1-6-250615")
    
    sf_api_key = os.getenv("SF_API_KEY")
    sf_base_url = os.getenv("SF_BASE_URL")
    sf_embedding_model = os.getenv("SF_EMBEDDING_MODEL", "Qwen/Qwen3-Embedding-8B")
    
    rerank_model = os.getenv("RERANK_MODEL", "")  # å¯é€‰é…ç½®
    
    # éªŒè¯å¿…éœ€é…ç½®
    if not ark_api_key:
        raise RuntimeError("ARK_API_KEY is not set!")
    if not sf_api_key:
        raise RuntimeError("SF_API_KEY is not set!")
    if not sf_base_url:
        raise RuntimeError("SF_BASE_URL is not set!")
    if not ark_base_url:
        raise RuntimeError("ARK_BASE_URL is not set!")
    
    # è¯»å– LightRAG æŸ¥è¯¢ä¼˜åŒ–å‚æ•°
    top_k = int(os.getenv("TOP_K", "20"))
    chunk_top_k = int(os.getenv("CHUNK_TOP_K", "10"))
    max_async = int(os.getenv("MAX_ASYNC", str(DEFAULT_MAX_ASYNC)))
    max_parallel_insert = int(os.getenv("MAX_PARALLEL_INSERT", "2"))
    max_entity_tokens = int(os.getenv("MAX_ENTITY_TOKENS", "6000"))
    max_relation_tokens = int(os.getenv("MAX_RELATION_TOKENS", "8000"))
    max_total_tokens = int(os.getenv("MAX_TOTAL_TOKENS", "30000"))
    
    # è¾“å‡ºé…ç½®ä¿¡æ¯
    logger.info("=" * 70)
    logger.info("ğŸ“Š RAG API é…ç½®æ€»è§ˆ")
    logger.info("=" * 70)
    logger.info(f"ğŸ¤– LLM: {ark_model}")
    logger.info(f"ğŸ”¤ Embedding: {sf_embedding_model} (dim={4096})")
    logger.info(f"ğŸ¯ Rerank: {rerank_model or 'Disabled'}")
    logger.info(f"ğŸ“ˆ Query: top_k={top_k}, chunk_top_k={chunk_top_k}, max_async={max_async}")
    logger.info(f"ğŸ’¾ Tokens: entity={max_entity_tokens}, relation={max_relation_tokens}, total={max_total_tokens}")
    logger.info(f"âš™ï¸  Concurrency: doc_processing=1, parallel_insert={max_parallel_insert}")
    logger.info("=" * 70)

    # 1. å®šä¹‰å…±äº«çš„ LLM å’Œ Embedding å‡½æ•°
    def llm_model_func(prompt, **kwargs):
        kwargs['enable_cot'] = False
        if 'system_prompt' not in kwargs:
            kwargs['system_prompt'] = DEFAULT_SYSTEM_PROMPT
        return openai_complete_if_cache(
            ark_model, prompt, api_key=ark_api_key, base_url=ark_base_url, **kwargs
        )

    embedding_func = EmbeddingFunc(
        embedding_dim=4096,  # Qwen/Qwen3-Embedding-8B å®é™…è¿”å› 4096 ç»´å‘é‡
        func=lambda texts: openai_embed(
            texts, model=sf_embedding_model, api_key=sf_api_key, base_url=sf_base_url
        ),
    )
    
    def vision_model_func(prompt, **kwargs):
        return openai_complete_if_cache(
            ark_model, prompt, api_key=ark_api_key, base_url=ark_base_url, **kwargs
        )
    
    # é…ç½® Rerank å‡½æ•°ï¼ˆå¯é€‰ï¼Œæå‡æ£€ç´¢ç›¸å…³æ€§ï¼‰
    rerank_func = None
    if rerank_model and cohere_rerank:
        rerank_func = partial(
            cohere_rerank,
            model=rerank_model,  # ä¾‹å¦‚ï¼šQwen/Qwen3-Reranker-8B
            api_key=sf_api_key,  # å¤ç”¨ç¡…åŸºæµåŠ¨çš„ API Key
            base_url=f"{sf_base_url}/rerank"  # ç¡…åŸºæµåŠ¨çš„ Rerank ç«¯ç‚¹
        )
        logger.info(f"âœ“ Rerank enabled with model: {rerank_model}")
    else:
        logger.info("âš  Rerank disabled (RERANK_MODEL not set or cohere_rerank unavailable)")

    # 2. åˆ›å»ºå•ä¸€ LightRAG å®ä¾‹ï¼ˆæ ¸å¿ƒçŸ¥è¯†å›¾è°±ï¼Œæ‰€æœ‰è§£æå™¨å…±äº«ï¼‰
    logger.info("Creating shared LightRAG instance...")

    # è¯»å–å¤–éƒ¨å­˜å‚¨é…ç½®
    use_external_storage = os.getenv("USE_EXTERNAL_STORAGE", "false").lower() == "true"
    kv_storage = os.getenv("KV_STORAGE", "JsonKVStorage")
    vector_storage = os.getenv("VECTOR_STORAGE", "NanoVectorDB")
    graph_storage = os.getenv("GRAPH_STORAGE", "NetworkXStorage")

    # æ ¹æ®é…ç½®åˆ›å»º LightRAG å®ä¾‹
    if use_external_storage:
        logger.info("=" * 70)
        logger.info("ğŸ”Œ Using external storage backends:")
        logger.info(f"   - KV Storage: {kv_storage}")
        logger.info(f"   - Vector Storage: {vector_storage}")
        logger.info(f"   - Graph Storage: {graph_storage}")
        logger.info("=" * 70)

        # å‡†å¤‡å­˜å‚¨é…ç½®
        storage_kwargs = {}

        # Redis KV å­˜å‚¨é…ç½®
        if kv_storage == "RedisKVStorage":
            redis_host = os.getenv("REDIS_HOST", "localhost")
            redis_port = int(os.getenv("REDIS_PORT", "6379"))
            redis_db = int(os.getenv("REDIS_DB", "0"))
            logger.info(f"   Redis: {redis_host}:{redis_port} (db={redis_db})")

            storage_kwargs["kv_storage"] = "RedisKVStorage"
            storage_kwargs["kv_storage_cls_kwargs"] = {
                "host": redis_host,
                "port": redis_port,
                "db": redis_db
            }
            # å¯é€‰ï¼šRedis å¯†ç 
            redis_password = os.getenv("REDIS_PASSWORD", "")
            if redis_password:
                storage_kwargs["kv_storage_cls_kwargs"]["password"] = redis_password

        # PostgreSQL å‘é‡å­˜å‚¨é…ç½®
        if vector_storage == "PGVectorStorage":
            postgres_host = os.getenv("POSTGRES_HOST", "localhost")
            postgres_port = int(os.getenv("POSTGRES_PORT", "5432"))
            postgres_db = os.getenv("POSTGRES_DB", "lightrag")
            postgres_user = os.getenv("POSTGRES_USER", "lightrag")
            logger.info(f"   PostgreSQL: {postgres_host}:{postgres_port}/{postgres_db}")

            storage_kwargs["vector_storage"] = "PGVectorStorage"
            storage_kwargs["vector_storage_cls_kwargs"] = {
                "host": postgres_host,
                "port": postgres_port,
                "database": postgres_db,
                "user": postgres_user,
                "password": os.getenv("POSTGRES_PASSWORD", "")
            }

        # Neo4j å›¾å­˜å‚¨é…ç½®
        if graph_storage == "Neo4JStorage":
            neo4j_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
            neo4j_username = os.getenv("NEO4J_USERNAME", "neo4j")
            logger.info(f"   Neo4j: {neo4j_uri}")

            storage_kwargs["graph_storage"] = "Neo4JStorage"
            storage_kwargs["graph_storage_cls_kwargs"] = {
                "uri": neo4j_uri,
                "user": neo4j_username,
                "password": os.getenv("NEO4J_PASSWORD", "")
            }

        global_lightrag_instance = LightRAG(
            working_dir="./rag_local_storage",  # ä»…ç”¨äºä¸´æ—¶æ–‡ä»¶
            llm_model_func=llm_model_func,
            embedding_func=embedding_func,
            llm_model_max_async=max_async,
            **storage_kwargs  # åº”ç”¨å¤–éƒ¨å­˜å‚¨é…ç½®
        )
    else:
        logger.info("=" * 70)
        logger.info("ğŸ“ Using local file storage (default)")
        logger.info("=" * 70)

        global_lightrag_instance = LightRAG(
            working_dir="./rag_local_storage",
            llm_model_func=llm_model_func,
            embedding_func=embedding_func,
            llm_model_max_async=max_async,  # ä¼˜åŒ–å¹¶å‘æ€§èƒ½ï¼ˆä» 4 æå‡åˆ° 8ï¼‰
        )
    
    # åˆå§‹åŒ– LightRAG å­˜å‚¨
    await global_lightrag_instance.initialize_storages()
    await initialize_pipeline_status()
    
    # é…ç½® Rerankï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if rerank_func:
        global_lightrag_instance.rerank_model_func = rerank_func
        logger.info("âœ“ LightRAG Rerank configured")
    
    logger.info("âœ“ Shared LightRAG instance created successfully")

    # 3. åˆ›å»º MinerU è§£æå™¨å®ä¾‹ï¼ˆå…±äº« LightRAGï¼‰
    config_mineru = RAGAnythingConfig(
        working_dir="./rag_local_storage",
        parser="mineru",  # å¼ºå¤§çš„å¤šæ¨¡æ€è§£æ
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )
    rag_instance_mineru = RAGAnything(
        config=config_mineru,
        lightrag=global_lightrag_instance,  # ä¼ å…¥å…±äº«çš„ LightRAG å®ä¾‹
        vision_model_func=vision_model_func,
    )
    logger.info("âœ“ MinerU parser initialized (shares LightRAG instance)")

    # 4. åˆ›å»º Docling è§£æå™¨å®ä¾‹ï¼ˆå…±äº« LightRAGï¼‰
    config_docling = RAGAnythingConfig(
        working_dir="./rag_local_storage",  # å…±äº«ç›¸åŒçš„ working_dir
        parser="docling",  # è½»é‡çº§è§£æå™¨
        enable_image_processing=False,  # Docling ä¸æ”¯æŒå¤šæ¨¡æ€
        enable_table_processing=False,
        enable_equation_processing=False,
    )
    rag_instance_docling = RAGAnything(
        config=config_docling,
        lightrag=global_lightrag_instance,  # ä¼ å…¥å…±äº«çš„ LightRAG å®ä¾‹
        vision_model_func=vision_model_func,
    )
    logger.info("âœ“ Docling parser initialized (shares LightRAG instance)")

    # 5. è®¾ç½®é»˜è®¤å®ä¾‹ä¸º MinerUï¼ˆå‘åå…¼å®¹ï¼‰
    rag_instance = rag_instance_mineru
    
    logger.info("=" * 70)
    logger.info("âœ… Architecture: Single LightRAG + Multiple Parsers")
    logger.info("   - Shared LightRAG: 1 instance (knowledge graph core)")
    logger.info("   - MinerU Parser: for complex multimodal documents")
    logger.info("   - Docling Parser: for simple documents")
    logger.info("   - Direct Query: bypass parsers for 95% text queries")
    logger.info("=" * 70)

    # 6. åˆå§‹åŒ–æ–‡ä»¶æœåŠ¡å’Œæ¸…ç†ä»»åŠ¡
    from src.file_url_service import get_file_service
    file_service = get_file_service()
    
    # å¯åŠ¨åå°æ–‡ä»¶æ¸…ç†ä»»åŠ¡
    cleanup_interval = int(os.getenv("FILE_CLEANUP_INTERVAL", "3600"))  # é»˜è®¤ 1 å°æ—¶
    cleanup_hours = int(os.getenv("FILE_CLEANUP_HOURS", "24"))  # é»˜è®¤ 24 å°æ—¶ä¿ç•™
    file_service.start_cleanup_task(interval_seconds=cleanup_interval, max_age_hours=cleanup_hours)
    logger.info(f"âœ“ File cleanup task started: interval={cleanup_interval}s, retention={cleanup_hours}h")

    # 7. å¯åŠ¨æ€§èƒ½ç›‘æ§
    from src.metrics import get_metrics_collector
    metrics_collector = get_metrics_collector()
    metrics_collector.start_system_monitoring(interval=60)  # æ¯ 60 ç§’é‡‡é›†ä¸€æ¬¡ç³»ç»ŸæŒ‡æ ‡
    logger.info("âœ“ Performance monitoring started")

    # 8. é¢„çƒ­ Workersï¼ˆå‡å°‘é¦–æ¬¡æŸ¥è¯¢å»¶è¿Ÿï¼‰
    import time
    import asyncio
    logger.info("=" * 70)
    logger.info("ğŸ”¥ Warming up Workers (Embedding + LLM)...")
    logger.info("=" * 70)
    warmup_start = time.time()

    try:
        # å¹¶è¡Œé¢„çƒ­Embeddingå’ŒLLM Workers
        warmup_tasks = []

        # é¢„çƒ­Embedding Workers
        async def warmup_embedding():
            try:
                test_embedding = await embedding_func(["warmup test query"])
                logger.info(f"âœ“ Embedding Workers warmed up ({len(test_embedding[0])} dimensions)")
                return True
            except Exception as e:
                logger.warning(f"âš ï¸  Embedding warmup failed: {e}")
                return False

        # é¢„çƒ­LLM Workers
        async def warmup_llm():
            try:
                test_response = await llm_model_func("Hello, respond with 'Hi'")
                logger.info(f"âœ“ LLM Workers warmed up (response: {len(test_response)} chars)")
                return True
            except Exception as e:
                logger.warning(f"âš ï¸  LLM warmup failed: {e}")
                return False

        warmup_tasks.append(warmup_embedding())
        warmup_tasks.append(warmup_llm())

        # å¹¶è¡Œæ‰§è¡Œé¢„çƒ­
        results = await asyncio.gather(*warmup_tasks, return_exceptions=True)

        warmup_elapsed = time.time() - warmup_start
        success_count = sum(1 for r in results if r is True)

        if success_count == len(warmup_tasks):
            logger.info(f"âœ… All Workers ready in {warmup_elapsed:.2f}s")
        else:
            logger.warning(f"âš ï¸  Partial warmup completed in {warmup_elapsed:.2f}s ({success_count}/{len(warmup_tasks)} succeeded)")
            logger.warning("   Workers will be initialized on first request")

    except Exception as e:
        logger.error(f"âŒ Worker warmup failed: {e}")
        logger.warning("   Workers will be initialized on first request")

    logger.info("=" * 70)

    yield  # åº”ç”¨è¿è¡ŒæœŸé—´ä¿æŒå®ä¾‹å¯ç”¨

    # å…³é—­æ—¶æ¸…ç†èµ„æº
    logger.info("Shutting down RAGAnything instance...")
    # å¦‚æœéœ€è¦æ¸…ç†èµ„æºï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 

# è·å– LightRAG å®ä¾‹çš„å‡½æ•°ï¼ˆç”¨äºæŸ¥è¯¢ï¼Œç›´æ¥è®¿é—®çŸ¥è¯†å›¾è°±ï¼‰
def get_lightrag_instance():
    """
    è·å–å…±äº«çš„ LightRAG å®ä¾‹ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰
    
    ä¼˜åŠ¿ï¼š
    - ç»•è¿‡è§£æå™¨ï¼Œç›´æ¥è®¿é—®çŸ¥è¯†å›¾è°±
    - é€‚åˆ 95% çš„çº¯æ–‡æœ¬æŸ¥è¯¢
    - æ€§èƒ½æ›´ä¼˜ï¼Œèµ„æºå ç”¨æ›´ä½
    
    Returns:
        LightRAG: å…±äº«çš„ LightRAG å®ä¾‹
    """
    return global_lightrag_instance

# è·å– RAG å®ä¾‹çš„å‡½æ•°ï¼ˆç”¨äºæ–‡æ¡£æ’å…¥ï¼Œéœ€è¦è§£æå™¨ï¼‰
def get_rag_instance(parser: str = "auto"):
    """
    è·å– RAGAnything å®ä¾‹ï¼ˆç”¨äºæ–‡æ¡£æ’å…¥ï¼‰
    
    Args:
        parser: è§£æå™¨ç±»å‹
            - "mineru": ä½¿ç”¨ MinerUï¼ˆå¼ºå¤§å¤šæ¨¡æ€ï¼Œå†…å­˜å ç”¨å¤§ï¼‰
            - "docling": ä½¿ç”¨ Doclingï¼ˆè½»é‡å¿«é€Ÿï¼Œå†…å­˜å ç”¨å°ï¼‰
            - "auto": è‡ªåŠ¨é€‰æ‹©ï¼ˆé»˜è®¤è¿”å› MinerUï¼‰
    
    Returns:
        RAGAnything: å¯¹åº”çš„è§£æå™¨å®ä¾‹ï¼ˆå…±äº« LightRAGï¼‰
    """
    if parser == "docling":
        return rag_instance_docling
    elif parser == "mineru":
        return rag_instance_mineru
    else:  # "auto" or default
        return rag_instance  # é»˜è®¤ MinerU

def select_parser_by_file(filename: str, file_size: int) -> str:
    """
    æ ¹æ®æ–‡ä»¶ç‰¹å¾æ™ºèƒ½é€‰æ‹©è§£æå™¨
    
    ç­–ç•¥ï¼š
    - çº¯æ–‡æœ¬ (.txt, .md) â†’ è¿”å› "mineru"ï¼ˆå®é™…ä¼šåœ¨å¤„ç†å‡½æ•°ä¸­ç›´æ¥æ’å…¥ LightRAGï¼Œä¸ç»è¿‡è§£æå™¨ï¼‰
    - å›¾ç‰‡æ–‡ä»¶ (.jpg, .png) â†’ MinerUï¼ˆOCRèƒ½åŠ›å¼ºï¼‰
    - PDF/Office å°æ–‡ä»¶ (< 500KB) â†’ Doclingï¼ˆå¿«é€Ÿï¼‰
    - PDF/Office å¤§æ–‡ä»¶ (> 500KB) â†’ MinerUï¼ˆæ›´å¼ºå¤§ï¼‰
    - å…¶ä»– â†’ MinerUï¼ˆé»˜è®¤ï¼‰
    
    æ³¨æ„ï¼š
    - Docling åªæ”¯æŒ PDF å’Œ Office æ ¼å¼ï¼ˆ.pdf, .docx, .xlsx, .pptx, .htmlï¼‰
    - çº¯æ–‡æœ¬æ–‡ä»¶ä¼šè¢«ç‰¹æ®Šå¤„ç†ï¼šç›´æ¥è¯»å–å†…å®¹å¹¶æ’å…¥ LightRAGï¼Œæ— éœ€è§£æå™¨
    
    Args:
        filename: æ–‡ä»¶å
        file_size: æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    
    Returns:
        "mineru" æˆ– "docling"
    """
    import os
    ext = os.path.splitext(filename)[1].lower()
    
    # å›¾ç‰‡æ–‡ä»¶ â†’ MinerUï¼ˆéœ€è¦ OCRï¼‰
    if ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']:
        return "mineru"
    
    # çº¯æ–‡æœ¬æ–‡ä»¶ â†’ MinerUï¼ˆDocling ä¸æ”¯æŒ .txtï¼‰
    if ext in ['.txt', '.md', '.markdown']:
        return "mineru"
    
    # PDF/Office å°æ–‡ä»¶ â†’ Doclingï¼ˆå¿«é€Ÿï¼‰
    if ext in ['.pdf', '.docx', '.xlsx', '.pptx', '.html', '.htm'] and file_size < 500 * 1024:  # < 500KB
        return "docling"
    
    # å¤§æ–‡ä»¶æˆ–å…¶ä»– â†’ MinerU
    return "mineru"
